# E2Eæ•°æ®é›†Seq2Seqè‡ªç„¶è¯­è¨€ç”Ÿæˆå®éªŒ



## 1. E2Eæ•°æ®é›†ä»‹ç»

E2Eæ•°æ®é›†æ˜¯ç”±$j.novikova$ç­‰äººä¸2017å¹´å‘è¡¨çš„ä¸€ä¸ªç”¨äºåœ¨é¤é¥®é¢†åŸŸè®­ç»ƒç«¯åˆ°ç«¯ã€æ•°æ®é©±åŠ¨çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆç³»ç»Ÿçš„æ•°æ®é›†ã€‚E2Eæ•°æ®é›†å«æœ‰50502æ¡æ•°æ®ç¤ºä¾‹ï¼Œå¹¶æŒ‰ç…§$ 76.5:8.5:15 $çš„æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æ•°æ®é›†ä¸­å«æœ‰ä¸¤ä¸ªéƒ¨åˆ†ï¼šæ„ä¹‰è¡¨ç¤ºï¼ˆMeaning Representationï¼‰ $MR$ å’Œ è‡ªç„¶è¯­è¨€å‚è€ƒæ–‡æœ¬ï¼ˆhuman reference textï¼‰ $ NL Reference $æˆ–$ ref $ã€‚

ä¸€ä¸ªæ•°æ®çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š

![image-20230524153413591](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/image-20230524153413591.png)

æ•°æ®é›†ä¸­çš„æ¯ä¸ªMRåŒ…æ‹¬æœ‰3è‡³8ä¸ªå±æ€§ï¼ˆä¹Ÿå«æ§½ï¼‰ï¼Œå¦‚_name,food, area, values_ç­‰ã€‚å±æ€§-å€¼çš„è¯¦ç»†æƒ…å†µå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š



![image-20230524153350124](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/image-20230524153350124.png)



## 2. æ•°æ®é›†å¤„ç†

### 2.1 åˆ†è¯å™¨Tokenizer

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸­ï¼Œ"Tokenizer"é€šå¸¸æŒ‡çš„æ˜¯åˆ†è¯å™¨ã€‚åˆ†è¯å™¨æ˜¯ä¸€ç§å°†æ–‡æœ¬å¥å­åˆ†å‰²æˆ**å•ä¸ªè¯è¯­**æˆ–**å­è¯**çš„å·¥å…·ã€‚åœ¨NLPä»»åŠ¡ä¸­ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘æˆ–å‘½åå®ä½“è¯†åˆ«ç­‰ï¼Œé¦–å…ˆéœ€è¦å°†åŸå§‹æ–‡æœ¬è½¬æ¢æˆè®¡ç®—æœºå¯ä»¥ç†è§£å’Œå¤„ç†çš„å½¢å¼ã€‚åˆ†è¯å™¨åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­èµ·åˆ°é‡è¦çš„ä½œç”¨ã€‚é€šè¿‡å°†å¥å­åˆ†å‰²æˆå•ä¸ªçš„è¯è¯­æˆ–å­è¯ï¼Œåˆ†è¯å™¨ä¸ºæ–‡æœ¬æä¾›äº†ä¸€ä¸ªåŸºæœ¬çš„å•ä½ï¼Œä½¿å¾—åç»­çš„å¤„ç†æ›´åŠ ç²¾ç¡®å’Œé«˜æ•ˆã€‚

æœ¬æ¬¡å®è·µæ„é€ åˆ†è¯å™¨æ—¶éœ€ä½¿ç”¨ä¸€ä¸ªè¯è¡¨è¿›è¡Œåˆå§‹åŒ–,å®ç°çš„åˆ†è¯å™¨ä¸­é™¤äº†åˆå§‹åŒ–å‡½æ•°å¤–å…±å®ç°äº†4ä¸ªæ–¹æ³•ï¼š

* index_to_tokenï¼šç»™å®šç¼–å·ï¼ŒæŸ¥æ‰¾è¯æ±‡è¡¨ä¸­å¯¹åº”çš„è¯

* token_to_indexï¼šç»™å®šä¸€ä¸ªè¯ï¼ŒæŸ¥è¯¢å…¶åœ¨è¯æ±‡è¡¨ä¸­çš„ä½ç½®ï¼Œè®¾ç½®é»˜è®¤å€¼ä¸ºä½é¢‘è¯[UNK]çš„ç¼–å·

* encodeï¼šå¯¹å­—ç¬¦ä¸²è¿›è¡Œç¼–ç 

  encodeå‡½æ•°çš„ç¼–ç å¦‚ä¸‹ï¼Œä¼ å…¥å‚æ•°ä¸º

  ```python
  def encode(self, tokens):
          """å¯¹å­—ç¬¦ä¸²ç¼–ç """
          # å¼€å§‹æ ‡è®°
          token_ids = [self.token_to_index('[BOS]'), ]
          for token in tokens:
              token_ids.append(self.token_to_index(token))
          # ç»“æŸæ ‡è®°
          token_ids.append(self.token_to_index('[EOS]'))
          return token_ids
  ```

  

* decodeï¼šå°†ç¼–ç è½¬æ¢æˆå­—ç¬¦ä¸²

  decodeå‡½æ•°çš„ç¼–ç å¦‚ä¸‹ï¼š

  ```python
  def decode(self, token_indexes):
          """ç»™å®šåºåˆ—çš„ç¼–å·ï¼Œè§£ææˆä¸ºå­—ç¬¦ä¸²"""
          # èµ·æ­¢æ ‡è®°å¤„ç†
          special_tokens = {'[BOS]', '[EOS]', '[PAD]'}
          # è§£æäº§ç”Ÿçš„å­—ç¬¦åˆ—è¡¨
          tokens = []
          for token_index in token_indexes:
              token = self.index_to_token(token_index)
              if token in special_tokens:
                  continue
              tokens.append(token)
          return " ".join(tokens)
  ```



### 2.2 æ•°æ®é¢„å¤„ç†ç±»

â€‹    åœ¨æ•°æ®é›†å¤„ç†ç±»ä¸­ï¼Œå°è£…äº†å…³äºæ•°æ®é›†é¢„å¤„ç†çš„æ–¹æ³•ï¼Œç”¨äºå°†æ–‡æœ¬å½¢å¼çš„E2Eæ•°æ®é›†å¤„ç†æˆä¸ºé€‚åˆäºæœ¬æ¬¡ä»»åŠ¡çš„æ•°æ®å½¢å¼ã€‚æ•°æ®é›†ä»¥.csvå½¢å¼çš„æ–‡ä»¶ç»™å‡ºï¼Œåœ¨æ•°æ®é¢„å¤„ç†ç±»ä¸­ä½¿ç”¨pandasç§‘å­¦æ•°æ®åº“æ¥è¯»å–ï¼ŒåŒæ—¶ï¼Œç”±äºE2Eæ•°æ®é›†åŒ…æ‹¬è®­ç»ƒé›†(trainset)ã€éªŒè¯é›†(devset)ä¸æµ‹è¯•é›†(testset)ä¸‰ä¸ªéƒ¨åˆ†ï¼Œè€Œè®­ç»ƒé›†ä¸éªŒè¯é›†å‡åŒ…å«ä¸¤åˆ—ï¼š$mr$ä¸$ref$ï¼Œæµ‹è¯•é›†åˆ™åªåŒ…å«$mr$(åŸæ•°æ®é›†ä¸­åˆ—åè¡¨ç¤ºä¸ºäº†MRï¼Œå¯é€‚å½“ä¿®æ”¹)ï¼Œä»è€Œéœ€è¦åˆ†ä¸åŒæƒ…å†µè¿›è¡Œè¯»å…¥ã€‚

#### 2.2.1 åˆå§‹åŒ–å‡½æ•°\_\_init\_\_ 

â€‹    è€ƒè™‘åˆ°è®­ç»ƒé›†ä¸éªŒè¯é›†å‡æ˜¯åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼Œä¸”æ•°æ®çš„ç»“æ„ç›¸åŒï¼Œæ•…åˆå§‹åŒ–å‡½æ•°\_\_init\_\_ä¸­è®¾ç½®`train_mod`è¿™ä¸€å‚æ•°ï¼Œç”¨äºåœ¨ä¸åŒçš„åœºæ™¯ä¸‹è¿›è¡Œä¸åŒçš„è¯»å–æ“ä½œã€‚åˆå§‹åŒ–å‡½æ•°çš„å¦å¤–ä¸¤ä¸ªå‚æ•°ä¸ºç»“æ„åŒ–æ–‡æœ¬ä¸­çš„å±æ€§çš„è¯å…¸`attributes_vocab`å’Œåˆ†è¯å™¨`tokenizer`ã€‚åˆå§‹åŒ–å‡½æ•°çš„ç¼–å†™å¦‚ä¸‹ï¼Œå…¶ä¸­ä½¿ç”¨åˆ°çš„åŠŸèƒ½å‡½æ•°`str2dict`åœ¨åç»­éƒ¨åˆ†åšè¯¦ç»†ä»‹ç»ï¼š

```python
    def __init__(self, file_path, train_mod=True, attributes_vocab=None, tokenizer=None):
        self.train_mod = train_mod
        df = pd.read_csv(file_path)
        
        if self.train_mod=='train' or self.train_mod=='valid':
            # è®­ç»ƒé›†ä¸éªŒè¯é›†è¯»å–
            self.mr = str2dict(df['mr'].values.tolist())  # type: list[dict]
            self.ref = df['ref'].values.tolist()  # type: list[str]
        elif self.train_mod=='test':
            self.mr = str2dict(df['mr'].values.tolist())
            self.ref = ['' for _ in range(len(self.mr))]
        else:
            print("Error! mode must be in ['train','valid','test']")
            exit(-2)

        self.raw_data_x = []  # å­˜å‚¨ç»“æ„åŒ–æ–‡æœ¬æ•°æ®mr, feature
        self.raw_data_y = []  # å­˜å‚¨ref, target
        self.lexicalizations = []  # å­˜å‚¨å»è¯åŒ–åŸè¯
        self.multi_data_y = {}  # ç»“æ„åŒ–æ–‡æœ¬å¯¹åº”çš„å¤šä¸ªref
```

â€‹    å¯¹äºæ•°æ®ä¸­çš„$mr$åˆ—ï¼Œå¦‚å‰æ–‡å¯¹æ•°æ®é›†çš„ä»‹ç»ï¼Œå…¶å½¢å¼ä¸ºè‹¥å¹²`Attribute[Value]`çš„ç±»ä¼¼é”®å€¼å¯¹çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œå¦‚"`name[Alimentum], area[city centre], familyFriendly[no]`"ç­‰ã€‚æˆ‘ä»¬å¸Œæœ›å°†è¿™æ ·å½¢å¼çš„æ•°æ®è½¬æ¢ä¸ºä¸€ä¸ªçœŸæ­£çš„é”®å€¼å¯¹å½¢å¼çš„å­—å…¸å½¢å¼çš„æ•°æ®ï¼Œäºæ˜¯ä½¿ç”¨å¦‚ä¸‹çš„åŠŸèƒ½å‡½æ•°`str2dict`ï¼Œå…¶è¾“å…¥æ˜¯ä¸€ä¸ªå­˜å‚¨å­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­å…·ä½“å†…å®¹åˆ™æ˜¯ç»“æ„åŒ–æ–‡æœ¬ä¸­$mr$åˆ—çš„æ•°æ®ã€‚ä»£ç ç¼–å†™ç»†èŠ‚ä»¥åŠpydocè¯´æ˜å¦‚ä¸‹ï¼š

```python
def str2dict(str_list):
    """å°†å­—ç¬¦ä¸²æ ¼å¼çš„ç»“æ„åŒ–æ–‡æœ¬(mr) å¤„ç†ä¸ºå­—å…¸æ ¼å¼
    Examples:
            >> str_list_test = ["name[The Wrestlers], eatType[coffee shop], food[English]"]
            >> str2dict(str_list_test)
            >> [{'name': 'The Wrestlers', 'eatType': 'coffee shop', 'food': 'English'}]
    """
    dict_list = []
    # åˆ†ç¦»å±æ€§(key)å’Œå€¼(value)
    map_keys = list(map(lambda x: x.split(', '), str_list))
    for map_key in map_keys:
        _dict = {}
        for item in map_key:  # è·å–é”®å€¼å¯¹ 'Attribute[value]'
            key = item.split('[')[0]
            value = item.split('[')[1].replace(']', '')
            _dict[key] = value
        dict_list.append(_dict)
    return dict_list
```



â€‹    æ­¤å¤–åœ¨æ•°æ®é¢„å¤„ç†ç±»ä¸­å®šä¹‰äº†å››ä¸ªåŠŸèƒ½å‡½æ•°ï¼Œå…¶å‡½æ•°åä»¥åŠåŠŸèƒ½æ¦‚è¿°å¦‚ä¸‹ï¼Œå…·ä½“å„å‡½æ•°çš„å®ç°ç»†èŠ‚å°†åœ¨åç»­éƒ¨åˆ†ä½œè¯¦ç»†ä»‹ç»ï¼š

* `preprocess`: å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œæ‰§è¡Œè¯æ±‡åŒ–ã€å»è¯åŒ–ç­‰æ“ä½œï¼Œå¤„ç†åå°†ç»“æœå­˜å…¥æœ¬ç±»çš„`raw_data_x`ç­‰æˆå‘˜å˜é‡ä¸­
* `build_vocab`:  åˆ©ç”¨æè¿°æ–‡æœ¬$ref$æ„å»ºè¯æ±‡è¡¨ï¼Œå¹¶åˆ©ç”¨æ„å»ºçš„è¯æ±‡è¡¨æ¥æ„å»ºåˆ†è¯å™¨`tokenizer`
* `build_attribute_vocab`: åˆ©ç”¨ç»“æ„åŒ–æ–‡æœ¬çš„$mr$åˆ—æ¥æ„å»ºå±æ€§çš„è¯æ±‡è¡¨ï¼Œä¸»è¦æ ¹æ®é”®çš„è¯é¢‘æ¥æ„å»º
* `sequence_padding`: å¯¹æ–‡æœ¬è¿›è¡Œå¡«å……ã€è¶…é•¿æˆªæ–­ç­‰æ“ä½œã€‚



â€‹    æœ€åå†äº\_\_init\_\_å‡½æ•°ä¸­è¿›è¡Œæœ€åçš„å¤„ç†ï¼Œå³æ ¹æ®ä¸åŒçš„æ•°æ®é›†è°ƒç”¨ä¸åŒçš„æ“ä½œå‡½æ•°å®Œæˆç±»çš„åˆå§‹åŒ–è¿‡ç¨‹ã€‚åœ¨åç»­çš„ä¸»å‡½æ•°è°ƒç”¨æ—¶ï¼Œé¦–å…ˆåŠ è½½è®­ç»ƒé›†ï¼Œä¹‹åå†åŠ è½½éªŒè¯é›†æµ‹è¯•é›†ã€‚åŠ è½½è®­ç»ƒé›†æ—¶ä¼šæ„å»ºå¾—åˆ°ä¸€ä¸ªè¯å…¸ã€åˆ†è¯å™¨ï¼Œè¿™ä¸ªè¯å…¸å’Œåˆ†è¯å™¨ç”¨ä½œåŠ è½½åä¸¤ä¸ªæ•°æ®é›†æ—¶ä¼ å…¥ã€‚

```python
        if self.train_mod=='train':
            self.build_attributes_vocab()   # æ„å»ºå±æ€§è¯å…¸
            self.preprocess()               # æ•°æ®é¢„å¤„ç†ã€å»è¯åŒ–
            self.build_vocab()              # æ„å»ºæ–‡æœ¬è¯å…¸
        else:
            if attributes_vocab is None or tokenizer is None:
                raise ValueError("For test set, attributes_vocab and tokenizer are necessary!")
            self.attributes_vocab = attributes_vocab
            self.key_num = len(self.attributes_vocab)
            self.tokenizer = tokenizer
            self.preprocess()
```



#### 2.2.2 æ•°æ®é¢„å¤„ç†å‡½æ•°preprocess

é¦–å…ˆï¼Œä»£ç é€šè¿‡éå†mrçš„å­—å…¸ï¼Œè·å–å±æ€§å’Œå¯¹åº”çš„å€¼ã€‚é€šè¿‡å±æ€§è¯å…¸ï¼ˆattributes_vocabï¼‰æ‰¾åˆ°å±æ€§åçš„ç¼–å·ï¼ˆkey_indexï¼‰ï¼Œå¹¶åœ¨å±æ€§åˆ—è¡¨ï¼ˆmr_dataï¼‰çš„å¯¹åº”ä½ç½®è®°å½•å±æ€§å€¼ã€‚å±æ€§åˆ—è¡¨çš„é•¿åº¦ä¸ºkey_numï¼Œä¸åŒä½ç½®å¯¹åº”ä¸åŒçš„å±æ€§ã€‚å…¶ä¸­ï¼Œå¦‚æœå±æ€§åæ˜¯'name'ï¼Œåˆ™å°†å¯¹åº”ä½ç½®çš„å±æ€§å€¼è®¾ç½®ä¸ºç‰¹æ®Šçš„NAME_TOKENï¼Œå¦‚æœå±æ€§åæ˜¯'near'ï¼Œåˆ™å°†å¯¹åº”ä½ç½®çš„å±æ€§å€¼è®¾ç½®ä¸ºç‰¹æ®Šçš„NEAR_TOKENã€‚å±æ€§å€¼ä¼šè¢«ä¿å­˜åœ¨lexåˆ—è¡¨ä¸­ï¼Œåˆ†åˆ«å¯¹åº”nameå’Œnearçš„å»è¯åŒ–åŸè¯ï¼Œè¿™é‡Œçš„å»è¯æ±‡åŒ–delexicalizationæ˜¯æŒ‡åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œå°†å…·ä½“çš„è¯è¯­æˆ–çŸ­è¯­æ›¿æ¢ä¸ºæŠ½è±¡çš„å ä½ç¬¦æˆ–ç¬¦å·çš„è¿‡ç¨‹ã€‚å®ƒæ˜¯å°†æ–‡æœ¬ä¸­çš„è¯æ±‡ä¿¡æ¯å»é™¤æˆ–æŠ½è±¡åŒ–çš„ä¸€ç§æ“ä½œã€‚

```python
def preprocess(self):
        for index in range(len(self.mr)):
            mr_data = [PAD_ID] * self.key_num
            lex = ['', '']  
            # æœ€ç»ˆçš„ç›®çš„æ˜¯å¾—åˆ°æè¿° mr çš„å®šé•¿åˆ—è¡¨
            for item in self.mr[index].items():
                key = item[0]
                value = item[1]
                key_index = self.attributes_vocab[key]

                # å°†ç»“æ„åŒ–æ–‡æœ¬mrè½¬æ¢ä¸ºå±æ€§åˆ—è¡¨å¹¶å»è¯åŒ–å¤„ç†
                if key == 'name':
                    mr_data[key_index] = NAME_TOKEN
                    lex[0] = value
                elif key == 'near':
                    mr_data[key_index] = NEAR_TOKEN
                    lex[1] = value
                else:
                    mr_data[key_index] = value
```



æ¥ä¸‹æ¥ï¼Œä»£ç å¤„ç†å‚è€ƒæ–‡æœ¬æ•°æ®ã€‚é¦–å…ˆï¼Œå°†ref_dataåˆå§‹åŒ–ä¸ºself.ref[index]çš„å€¼ï¼Œå¦‚æœè¯¥å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºåŒ…å«ä¸€ä¸ªç©ºå­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚ç„¶åï¼Œå¦‚æœlex[0]éç©ºï¼ˆå³nameå­˜åœ¨ï¼‰ï¼Œåˆ™å°†ref_dataä¸­çš„lex[0]æ›¿æ¢ä¸ºNAME_TOKENï¼›å¦‚æœlex[1]éç©ºï¼ˆå³nearå­˜åœ¨ï¼‰ï¼Œåˆ™å°†ref_dataä¸­çš„lex[1]æ›¿æ¢ä¸ºNEAR_TOKENã€‚æœ€åï¼Œä»£ç ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤å¥å­ä¸­çš„æ ‡ç‚¹ç¬¦å·ï¼Œå¹¶å°†å¤„ç†åçš„å¥å­åˆ‡åˆ†æˆå•è¯åˆ—è¡¨ã€‚

```python
			# å°†refä¹Ÿå¤„ç†æˆåˆ—è¡¨
            ref_data = self.ref[index]
            if self.train_mod:
                if lex[0]:
                    ref_data = ref_data.replace(lex[0], NAME_TOKEN)
                if lex[1]:
                    ref_data = ref_data.replace(lex[1], NEAR_TOKEN)
                # æ­£åˆ™è¡¨è¾¾å¼å»é™¤å¥å­ä¸­çš„æ ‡ç‚¹
                ref_data = list(map(lambda x: re.split(r"([.,!?\"':;)(])", x)[0], ref_data.split()))
            else:
                ref_data = ['']
```



æœ€åï¼Œä»£ç å°†å¤„ç†åçš„ç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼ˆmr_dataï¼‰ã€å‚è€ƒæ–‡æœ¬æ•°æ®ï¼ˆref_dataï¼‰ä»¥åŠå»è¯åŒ–åŸè¯ï¼ˆlexï¼‰è¿½åŠ åˆ°ç›¸åº”çš„åˆ—è¡¨ä¸­ã€‚å¦‚æœå¤šä¸ªç»“æ„åŒ–æ–‡æœ¬å…·æœ‰ç›¸åŒçš„mr_data_strï¼ˆé€šè¿‡å°†mr_dataè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰ï¼Œåˆ™å°†ç›¸åº”çš„å‚è€ƒæ–‡æœ¬è¿½åŠ åˆ°å­—å…¸çš„å€¼åˆ—è¡¨ä¸­ã€‚å¦‚æœmr_data_strä¸å­˜åœ¨äºå­—å…¸çš„é”®ä¸­ï¼Œåˆ™å°†mr_data_strä½œä¸ºé”®ï¼Œå°†å‚è€ƒæ–‡æœ¬ä½œä¸ºå€¼åˆ—è¡¨æ·»åŠ åˆ°å­—å…¸ä¸­ã€‚

```python
            self.raw_data_x.append(mr_data)
            self.raw_data_y.append(ref_data)
            self.lexicalizations.append(lex)
            mr_data_str = ' '.join(mr_data)
            if mr_data_str in self.multi_data_y.keys():
                self.multi_data_y[mr_data_str].append(self.ref[index])
            else:
                self.multi_data_y[mr_data_str] = [self.ref[index]]
```



#### 2.2.2 æ„å»ºè¯æ±‡è¡¨å‡½æ•°build_vocab

æ„å»ºè¯æ±‡è¡¨ï¼Œå¹¶åˆ©ç”¨æ„å»ºçš„è¯æ±‡è¡¨æ¥æ„å»ºåˆ†è¯å™¨`tokenizer`ã€‚

é¦–å…ˆä½¿ç”¨Counterå¯¹è±¡ç»Ÿè®¡self.raw_data_xå’Œself.raw_data_yä¸­çš„è¯é¢‘ã€‚self.raw_data_xæ˜¯ä¸€ä¸ªåŒ…å«ç»“æ„åŒ–æ–‡æœ¬æ•°æ®çš„åˆ—è¡¨ï¼Œself.raw_data_yæ˜¯ä¸€ä¸ªåŒ…å«å‚è€ƒæ–‡æœ¬æ•°æ®çš„åˆ—è¡¨ã€‚

æ¥ä¸‹æ¥ï¼Œå°†è¯é¢‘ç»Ÿè®¡ç»“æœæŒ‰ç…§è¯é¢‘è¿›è¡Œæ’åºï¼Œå¾—åˆ°tokens_count_listï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«è¯å’Œå¯¹åº”è¯é¢‘çš„å…ƒç»„ã€‚

ç„¶åï¼Œå°†ç‰¹æ®Šçš„æ ‡è®°ç¬¦å·ï¼ˆ'[PAD]', '[BOS]', '[EOS]', '[UNK]'ï¼‰æ·»åŠ åˆ°tokens_listä¸­ï¼Œæ ‡è®°ç¬¦å·ä¾æ¬¡è¡¨ç¤ºå¡«å……tokenã€å¼€å§‹tokenã€ç»“æŸtokenã€æœªçŸ¥æˆ–ä½é¢‘tokenï¼Œå¹¶å°†tokens_count_listä¸­çš„è¯æŒ‰ç…§è¯é¢‘æ’åºåä¾æ¬¡æ·»åŠ åˆ°tokens_listä¸­ã€‚

æœ€ååˆ›å»ºä¸€ä¸ªtoken_index_dictå­—å…¸ï¼Œå…¶ä¸­é”®ä¸ºè¯ï¼ˆtokens_listä¸­çš„å…ƒç´ ï¼‰ï¼Œå€¼ä¸ºè¯¥è¯åœ¨tokens_listä¸­çš„ç´¢å¼•ï¼Œå¹¶ä½¿ç”¨token_index_dictæ¥æ„å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼ˆTokenizerï¼‰ï¼Œè¯¥åˆ†è¯å™¨å°†è¯è½¬æ¢ä¸ºå¯¹åº”çš„ç´¢å¼•å€¼ã€‚è¿™ä¸ªåˆ†è¯å™¨å¯ä»¥ç”¨äºå°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—è¡¨ç¤ºå½¢å¼ï¼ŒTokenizerçš„å®ç°è§å‰æ–‡ã€‚

```python
def build_vocab(self):
    """æ„å»ºè¯å…¸"""
    # ç»Ÿè®¡è¯é¢‘
    counter = Counter()
    for item in self.raw_data_x:
        counter.update(item)
    for item in self.raw_data_y:
        counter.update(item)
    # æŒ‰ç…§è¯é¢‘è¿›è¡Œæ’åº
    tokens_count_list = [(token, count) for token, count in counter.items()]
    tokens_count_list = sorted(tokens_count_list, key=lambda x: -x[1])
    # å»é™¤è¯é¢‘çš„wordåˆ—è¡¨
    tokens_list = ['[PAD]', '[BOS]', '[EOS]', '[UNK]'] + [token for token, count in tokens_count_list]

    token_index_dict = dict(zip(tokens_list, range(len(tokens_list))))
    # å»ºç«‹åˆ†è¯å™¨
    self.tokenizer = Tokenizer(token_index_dict)
```



#### 2.2.3 æ„å»ºå±æ€§è¯å…¸å‡½æ•°build_attributes_vocab

`build_attributes_vocab`å‡½æ•°ç”¨äºæ ¹æ®æ•°æ®ä¸­çš„ç»“æ„åŒ–æ–‡æœ¬$ mr $ä¸­çš„å±æ€§å€¼æ¥æ„å»ºå±æ€§è¯å…¸ï¼ˆattributes_vocabï¼‰ï¼Œç”¨äºè®°å½•ç»“æ„åŒ–æ–‡æœ¬ä¸­çš„å±æ€§åï¼ˆkeyï¼‰ä»¥åŠå¯¹åº”çš„ç´¢å¼•ã€‚

é¦–å…ˆä½¿ç”¨mapå‡½æ•°å’Œlambdaè¡¨è¾¾å¼ï¼Œå¯¹self.mrä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆå­—å…¸ï¼‰åº”ç”¨list(x.keys())ï¼Œå°†æ¯ä¸ªå­—å…¸çš„é”®è½¬æ¢ä¸ºåˆ—è¡¨ã€‚è¿™æ ·å¾—åˆ°çš„mr_keyæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨åŒ…å«ä¸€ä¸ªç»“æ„åŒ–æ–‡æœ¬çš„æ‰€æœ‰å±æ€§åã€‚

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨Counterå¯¹è±¡å¯¹mr_keyä¸­çš„å±æ€§åè¿›è¡Œè¯é¢‘ç»Ÿè®¡ï¼Œä¹‹åæŒ‰ç…§è¯é¢‘è¿›è¡Œæ’åºï¼Œå¾—åˆ°keys_count_listï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«å±æ€§åå’Œå¯¹åº”è¯é¢‘çš„å…ƒç»„ã€‚

ç„¶åï¼Œå°†keys_count_listä¸­çš„å±æ€§åæŒ‰ç…§è¯é¢‘æ’åºåä¾æ¬¡æ·»åŠ åˆ°keys_listä¸­ã€‚

æœ€åï¼Œä»£ç åˆ›å»ºä¸€ä¸ªattributes_vocabå­—å…¸ï¼Œå…¶ä¸­é”®ä¸ºå±æ€§åï¼ˆkeys_listä¸­çš„å…ƒç´ ï¼‰ï¼Œå€¼ä¸ºè¯¥å±æ€§ååœ¨keys_listä¸­çš„ç´¢å¼•ï¼Œå¹¶ä»£ç å°†attributes_vocabçš„é•¿åº¦èµ‹å€¼ç»™self.key_numï¼Œè¡¨ç¤ºå±æ€§è¯å…¸ä¸­ä¸åŒå±æ€§çš„æ•°é‡ã€‚

```python
    def build_attributes_vocab(self):
        """æ„å»ºå±æ€§è¯å…¸ï¼Œå¯¹mrå­—æ®µä¸­çš„key ç»Ÿè®¡è¯é¢‘"""
        mr_key = list(map(lambda x: list(x.keys()), self.mr))  # type: list[list[str]]
        # print(mr_key)
        # è¯é¢‘ç»Ÿè®¡
        counter = Counter()
        for item in mr_key:
            counter.update(item)
        # æŒ‰ç…§è¯é¢‘è¿›è¡Œæ’åº
        keys_count_list = [(key, count) for key, count in counter.items()]
        keys_count_list = sorted(keys_count_list, key=lambda x: -x[1])

        # å»é™¤è¯é¢‘çš„keyåˆ—è¡¨
        keys_list = [key for key, count in keys_count_list]
        self.attributes_vocab = dict(zip(keys_list, range(len(keys_list))))
        self.key_num = len(self.attributes_vocab)
```



#### 2.2.4 æ•°æ®å¡«å……å‡½æ•°sequence_padding

æ•°æ®å¡«å……å‡½æ•°ç”¨äºå¯¹æ•°æ®è¿›è¡Œå¡«å……å’Œæˆªæ–­ï¼Œç¡®ä¿æ•°æ®å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚å‡½æ•°æ¥å—ä¸‰ä¸ªå‚æ•°ï¼š

* `data_`ï¼šå¾…å¡«å……çš„æ•°æ®

* `max_len`ï¼šæœ€å¤§é•¿åº¦ï¼Œé»˜è®¤å€¼ä¸ºconfig.max_sentence_length

* `padding`ï¼šå¡«å……çš„æ ‡è®°ï¼Œé»˜è®¤ä¸ºNone

é¦–å…ˆï¼Œåˆ¤æ–­å¦‚æœpaddingä¸ºNoneï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºç‰¹æ®Šæ ‡è®°ç¬¦å·'[PAD]'åœ¨åˆ†è¯å™¨ï¼ˆtokenizerï¼‰ä¸­å¯¹åº”çš„ç´¢å¼•ã€‚è¿™æ ·å¯ä»¥ä¿è¯åœ¨å¡«å……æ—¶ä½¿ç”¨ç›¸åŒçš„å¡«å……æ ‡è®°ã€‚ä¹‹åå°†paddingèµ‹å€¼ç»™self.paddingï¼Œä»¥ä¾¿åœ¨å…¶ä»–æ–¹æ³•ä¸­å¯ä»¥è®¿é—®åˆ°å¡«å……æ ‡è®°ã€‚

ä¹‹åï¼Œä»£ç è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦ï¼ˆpadding_lengthï¼‰ï¼Œå³max_lenå‡å»æ•°æ®data_çš„é•¿åº¦ï¼Œå¹¶æ ¹æ®å¡«å……é•¿åº¦çš„æƒ…å†µè¿›è¡Œå¡«å……æ“ä½œã€‚å¦‚æœpadding_lengthå¤§äº0ï¼Œå³æ•°æ®é•¿åº¦å°äºmax_lenï¼Œå°±åœ¨æ•°æ®çš„æœ«å°¾æ·»åŠ paddingå…ƒç´ ï¼ˆå¡«å……æ ‡è®°ï¼‰è‹¥å¹²æ¬¡ï¼Œä½¿æ•°æ®é•¿åº¦è¾¾åˆ°max_lenã€‚å¦‚æœpadding_lengthå°äºç­‰äº0ï¼Œå³æ•°æ®é•¿åº¦å¤§äºç­‰äºmax_lenï¼Œå°±æˆªå–æ•°æ®çš„å‰max_lenä¸ªå…ƒç´ ã€‚æœ€åä»£ç è¿”å›å¡«å……åçš„æ•°æ®ï¼ˆoutputsï¼‰ã€‚

```python
    def sequence_padding(self, data_, max_len=config.max_sentence_length, padding=None):
        """æ•°æ®å¡«å……"""
        if padding is None:
            padding = self.tokenizer.token_to_index('[PAD]')
        self.padding = padding
        # å¼€å§‹å¡«å……
        padding_length = max_len - len(data_)

        if padding_length > 0:
            outputs = data_ + [padding] * padding_length
        else:
            outputs = data_[:max_len]
        return outputs
```



### 2.3 PyTorchæ•°æ®åŠ è½½ç±»

æ­¤éƒ¨åˆ†å®šä¹‰äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„PyTorchæ•°æ®åŠ è½½ç±»`E2EDataset`ï¼Œç”¨äºåŠ è½½æ•°æ®å¹¶å‡†å¤‡è¿›è¡Œè®­ç»ƒæˆ–æ¨æ–­ã€‚

åœ¨ç±»çš„åˆå§‹åŒ–æ–¹æ³•`__init__`ä¸­ï¼Œæ¥æ”¶ä»¥ä¸‹å‚æ•°ï¼šfile_pathï¼ˆæ•°æ®æ–‡ä»¶è·¯å¾„ï¼‰ã€train_modï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰ã€attributes_vocabï¼ˆå±æ€§è¯å…¸ï¼Œé»˜è®¤ä¸ºNoneï¼‰ã€tokenizerï¼ˆåˆ†è¯å™¨ï¼Œé»˜è®¤ä¸ºNoneï¼‰ã€‚

åˆå§‹åŒ–æ–¹æ³•ä¸­åˆ›å»ºäº†ä¸€ä¸ª`DataProcess`ç±»çš„å¯¹è±¡ï¼ˆdataProcessorï¼‰ï¼Œå¹¶å°†è¾“å…¥çš„å‚æ•°ä¼ é€’ç»™è¯¥å¯¹è±¡è¿›è¡Œæ•°æ®å¤„ç†ã€‚é€šè¿‡dataProcessorå¯ä»¥è·å¾—å¤„ç†åçš„æ•°æ®å’Œç›¸å…³ä¿¡æ¯ï¼Œå¦‚self.refï¼ˆå‚è€ƒæ–‡æœ¬æ•°æ®ï¼‰ã€self.attributes_vocabï¼ˆå±æ€§è¯å…¸ï¼‰ã€self.tokenizerï¼ˆåˆ†è¯å™¨ï¼‰ã€self.raw_data_xï¼ˆç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼‰ã€self.raw_data_yï¼ˆå‚è€ƒæ–‡æœ¬æ•°æ®ï¼‰å’Œself.multi_data_yï¼ˆå¤šä¸ªå‚è€ƒæ–‡æœ¬æ•°æ®çš„å­—å…¸ï¼‰ã€‚

ç±»ä¸­å®šä¹‰äº†ä¸¤ä¸ªæ–¹æ³•ï¼š`__len__`å’Œ`__getitem__`ã€‚`__len__`æ–¹æ³•è¿”å›æ•°æ®é›†çš„é•¿åº¦ï¼Œå³å‚è€ƒæ–‡æœ¬æ•°æ®çš„æ•°é‡ã€‚`__getitem__`æ–¹æ³•ç”¨äºè·å–æŒ‡å®šç´¢å¼•çš„æ•°æ®æ ·æœ¬ã€‚é¦–å…ˆï¼Œæ ¹æ®ç´¢å¼•è·å–å¯¹åº”çš„åŸå§‹ç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼ˆraw_data_xï¼‰å’Œå‚è€ƒæ–‡æœ¬æ•°æ®ï¼ˆraw_data_yï¼‰ã€‚ç„¶åï¼Œä½¿ç”¨dataProcessorä¸­çš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼ˆencodeï¼‰ï¼Œå¹¶ä½¿ç”¨dataProcessorä¸­çš„æ•°æ®å¡«å……æ–¹æ³•ï¼ˆsequence_paddingï¼‰å¯¹ç¼–ç åçš„æ•°æ®è¿›è¡Œå¡«å……ã€‚å¡«å……åçš„æ•°æ®è¢«è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼ˆnp.arrayï¼‰ã€‚æœ€åï¼Œå¦‚æœè®­ç»ƒæ¨¡å¼ä¸º'train'ï¼Œåˆ™è¿”å›ç»“æ„åŒ–æ–‡æœ¬æ•°æ®å’Œå‚è€ƒæ–‡æœ¬æ•°æ®ã€‚å¦åˆ™ï¼Œè¿”å›ç»“æ„åŒ–æ–‡æœ¬æ•°æ®ã€å‚è€ƒæ–‡æœ¬æ•°æ®ã€å»è¯åŒ–åŸè¯ï¼ˆlexï¼‰å’Œå¤šä¸ªå‚è€ƒæ–‡æœ¬æ•°æ®çš„å­—å…¸ï¼ˆmulti_yï¼‰ã€‚

```python
    def __getitem__(self, index):
        x = np.array(self.dataProcessor.sequence_padding(
            self.dataProcessor.tokenizer.encode(self.raw_data_x[index]), 
            self.max_mr_len))
        y = np.array(self.dataProcessor.sequence_padding(
            self.dataProcessor.tokenizer.encode(self.raw_data_y[index]), 
            self.max_ref_len))
        if self.train_mod == 'train':
            return x, y
        else:
            lex = self.dataProcessor.lexicalizations[index]
            multi_y = self.multi_data_y[' '.join(self.raw_data_x[index])]
            return x, y, lex, multi_y
```



## 3. æ­å»ºSeq2Seqæ¨¡å‹

æ­¤éƒ¨åˆ†è¯¦ç»†ä»£ç è§Model.pyæ–‡ä»¶

Seq2Seqæ¨¡å‹æ˜¯ç”¨äºå¯å˜é•¿åº¦çš„è¾“å…¥åºåˆ—åˆ°å¯å˜é•¿åº¦çš„è¾“å‡ºåºåˆ—ä»»åŠ¡çš„ç»å…¸æ¨¡å‹ï¼Œå¸¸è§äºæœºå™¨ç¿»è¯‘ç­‰å…¸å‹ä»»åŠ¡ä¸­ã€‚Seq2Seqæ¨¡å‹æ˜¯ä¸€ç§ç»å…¸çš„Encoder-Decoderç»“æ„ï¼Œç»“æ„ä¸­åŒ…å«ä¸€ä¸ªç¼–ç å™¨ä¸ä¸€ä¸ªè§£ç å™¨ã€‚ç¼–ç å™¨ä½¿â½¤â»“åº¦å¯å˜çš„åºåˆ—ä½œä¸ºè¾“â¼Šï¼Œå°†å…¶è½¬æ¢ä¸ºå›ºå®šå½¢çŠ¶çš„éšçŠ¶æ€ã€‚å³è¾“â¼Šåºåˆ—çš„ä¿¡æ¯è¢«ç¼–ç åˆ°å¾ªç¯ç¥ç»â½¹ç»œç¼–ç å™¨çš„éšçŠ¶æ€ä¸­ã€‚ä¸ºäº†è¿ç»­â½£æˆè¾“å‡ºåºåˆ—çš„è¯å…ƒï¼Œç‹¬â½´çš„è§£ç å™¨æ˜¯åŸºäºè¾“â¼Šåºåˆ—çš„ç¼–ç ä¿¡æ¯å’Œè¾“å‡ºåºåˆ—å·²ç»çœ‹â»…çš„æˆ–è€…â½£æˆçš„è¯å…ƒæ¥é¢„æµ‹ä¸‹â¼€ä¸ªè¯å…ƒã€‚

![image-20230602152421745](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/image-20230602152421745.png)



### 3.1 Encoder

å¦‚å‰æ‰€è¿°ï¼Œç¼–ç å™¨çš„ä½œç”¨æ˜¯å°†â»“åº¦å¯å˜çš„è¾“â¼Šåºåˆ—ç¼–ç æˆâ¼€ä¸ªâ€œçŠ¶æ€â€ï¼Œä»¥ä¾¿åç»­å¯¹è¯¥çŠ¶æ€è¿›â¾è§£ç ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œç¼–ç å™¨å°†â»“åº¦å¯å˜çš„è¾“â¼Šåºåˆ—è½¬æ¢æˆå½¢çŠ¶å›ºå®šçš„ä¸Šä¸‹â½‚å˜é‡cï¼Œå¹¶ä¸”å°†è¾“â¼Šåºåˆ—çš„ä¿¡æ¯åœ¨è¯¥ä¸Šä¸‹â½‚å˜é‡ä¸­è¿›â¾ç¼–ç ã€‚å—ç®—åŠ›çš„é™åˆ¶ï¼Œè¿™é‡Œåªä½¿ç”¨çº¿æ€§å±‚æ¥å®ç°ä¸€ä¸ªç®€å•çš„Encoderã€‚

Encoderçš„æ„é€ å‡½æ•°æ¥å—ä¸¤ä¸ªå‚æ•°ï¼š`input_size`å’Œ`hidden_size`ï¼Œåˆ†åˆ«è¡¨ç¤ºè¾“å…¥ç‰¹å¾çš„ç»´åº¦å’Œç¼–ç å™¨éšè—å±‚çš„ç»´åº¦ã€‚

åœ¨`__init__`å‡½æ•°ä¸­ï¼Œä»£ç å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§å±‚`self.W`ï¼Œå®ƒä½¿ç”¨`nn.Linear`å°†è¾“å…¥ç‰¹å¾çš„ç»´åº¦è½¬æ¢ä¸ºéšè—å±‚çš„ç»´åº¦ã€‚`nn.Linear`æ¥å—ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, input_size)`çš„å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªç»è¿‡çº¿æ€§å˜æ¢åå½¢çŠ¶ä¸º`(batch_size, out_features)`çš„å¼ é‡ã€‚åŒæ—¶å®šä¹‰ä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°`self.relu`ã€‚

```python
    def __init__(self, input_size, hidden_size):
        super(Encoder, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.W = nn.Linear(in_features=self.input_size,
                           out_features=self.hidden_size)
        self.relu = nn.ReLU()
```



Encoderçš„çš„å‰å‘ä¼ æ’­å‡½æ•°ã€‚å®ƒæ¥å—çš„è¾“å…¥æ˜¯ä¸€ä¸ªå·²ç»ç»è¿‡è¯åµŒå…¥å¤„ç†çš„å¼ é‡`input_embedded`ï¼Œå½¢çŠ¶ä¸º`[seq_len, batch_size, embed_dim]`ã€‚åœ¨Seq2Seqæ¨¡å‹æ„å»ºæ—¶ï¼Œæ¥å—çš„è¾“å…¥ä¸ºç»è¿‡reshapeåçš„åŸå§‹å¼ é‡æ•°æ®ï¼Œå½¢çŠ¶ä¸º`[seq_len, batch_size]`,å¯¹å…¶è¿›è¡Œè¯åµŒå…¥åè¿›å…¥Encoderè¿›è¡Œç¼–ç ã€‚

é¦–å…ˆï¼Œä»£ç é€šè¿‡`input_embedded.size()`è·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶ä¿¡æ¯ï¼Œå¹¶å°†å…¶åˆ†åˆ«èµ‹å€¼ç»™`seq_len`ã€`batch_size`å’Œ`embed_dim`ã€‚æ¥ä¸‹æ¥ï¼Œä»£ç å°†è¾“å…¥å¼ é‡è¿›è¡Œé‡å¡‘ï¼Œå°†å…¶å½¢çŠ¶å˜ä¸º`[seq_len*batch_size, embed_dim]`ï¼Œç„¶åé€šè¿‡çº¿æ€§å±‚`self.W`å’ŒReLUæ¿€æ´»å‡½æ•°`self.relu`å¯¹è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œå¾—åˆ°è¾“å‡ºå¼ é‡`outputs`ã€‚æœ€åï¼Œä»£ç å°†`outputs`é‡æ–°è°ƒæ•´ä¸ºå½¢çŠ¶ä¸º`[seq_len, batch_size, -1]`çš„å¼ é‡ï¼Œå¹¶é€šè¿‡`torch.sum`å¯¹å…¶è¿›è¡Œæ±‚å’Œæ“ä½œï¼Œå¾—åˆ°ç¼–ç å™¨çš„éšè—çŠ¶æ€`decoder_hidden`ã€‚æœ€ç»ˆï¼Œå‡½æ•°è¿”å›`outputs`å’Œç»è¿‡`unsqueeze(0)`æ“ä½œåçš„`decoder_hidden`ã€‚

```python
    def forward(self, input_embedded):
        seq_len, batch_size, embed_dim = input_embedded.size()
        # å°†è¯åµŒå…¥çš„è¾“å…¥ reshape ä¸º [seq_len*batch_size, embed_dim]
        outputs = self.relu(self.W(input_embedded.view(-1, embed_dim)))
        outputs = outputs.view(seq_len, batch_size, -1)
        decoder_hidden = torch.sum(outputs, 0)
        return outputs, decoder_hidden.unsqueeze(0)
```



éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`input_size`éœ€è¦å’Œ`embed_dim`ç›¸ç­‰ã€‚



### 3.2 Decoder

å®ç°è§£ç å™¨çš„ç»“æ„ã€‚åœ¨ç±»çš„åˆå§‹åŒ–æ–¹æ³•`__init__`ä¸­ï¼Œæ¥æ”¶ä»¥ä¸‹å‚æ•°ï¼š

* input_sizeï¼ˆè§£ç å™¨è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼Œå³è¾“å…¥å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦å¤§å°ï¼‰
* hidden_sizeï¼ˆè§£ç å™¨éšè—çŠ¶æ€çš„ç»´åº¦ï¼Œå³GRUå•å…ƒçš„è¾“å‡ºå¤§å°ï¼‰ã€output_sizeï¼ˆè§£ç å™¨è¾“å‡ºç»´åº¦ï¼Œå³ç›®æ ‡ï¼ˆrefæ–‡æœ¬ï¼‰çš„è¯è¡¨å¤§å°ï¼‰
* embedding_dimï¼ˆè¯åµŒå…¥ç»´åº¦ï¼‰
* encoder_hidden_sizeï¼ˆç¼–ç å™¨éšè—å±‚è¾“å‡ºç»´åº¦ï¼‰ã€‚

åˆå§‹åŒ–æ–¹æ³•ä¸­å®šä¹‰äº†è§£ç å™¨çš„å„ä¸ªç»„ä»¶ï¼šä¸€ä¸ªGRUå±‚ï¼ˆself.rnnï¼‰ï¼Œä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆself.attnï¼‰ï¼Œä¸€ä¸ªçº¿æ€§å±‚ï¼ˆself.W_combineï¼‰ï¼Œä¸€ä¸ªçº¿æ€§å±‚ï¼ˆself.W_outï¼‰ï¼Œå’Œä¸€ä¸ªå¯¹æ•°softmaxå‡½æ•°ï¼ˆself.log_softmaxï¼‰ã€‚

```python
class Decoder(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, embedding_dim, encoder_hidden_size):
        super(Decoder, self).__init__()
        self.rnn = nn.GRU(input_size, hidden_size, bidirectional=False)
        self.attn = Attention(encoder_hidden_size, hidden_size)
        self.W_combine = nn.Linear(embedding_dim + encoder_hidden_size, hidden_size)
        self.W_out = nn.Linear(hidden_size, output_size)
        self.log_softmax = nn.LogSoftmax(dim=1)
```



åœ¨å‰å‘ä¼ æ’­æ–¹æ³•ï¼ˆforwardï¼‰ä¸­ï¼Œæ¥æ”¶ä¸‰ä¸ªè¾“å…¥å‚æ•°ï¼šprev_y_batchï¼ˆå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼‰ã€prev_h_batchï¼ˆå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼‰ã€encoder_outputs_batchï¼ˆç¼–ç å™¨çš„è¾“å‡ºï¼‰ã€‚

é¦–å…ˆï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼ˆself.attnï¼‰è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆattn_weightsï¼‰ï¼Œå¹¶åˆ©ç”¨è¿™äº›æƒé‡å¯¹ç¼–ç å™¨çš„è¾“å‡ºï¼ˆencoder_outputs_batchï¼‰è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontextï¼‰ã€‚

ç„¶åï¼Œå°†å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼ˆprev_y_batchï¼‰å’Œä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontextï¼‰è¿›è¡Œæ‹¼æ¥ï¼Œå½¢æˆæ–°çš„è¾“å…¥å‘é‡ï¼ˆy_ctxï¼‰ã€‚

æ¥ä¸‹æ¥ï¼Œå°†æ–°çš„è¾“å…¥å‘é‡ï¼ˆy_ctxï¼‰é€šè¿‡çº¿æ€§å±‚ï¼ˆself.W_combineï¼‰è¿›è¡Œå˜æ¢ï¼Œå¾—åˆ°GRUçš„è¾“å…¥ï¼ˆrnn_inputï¼‰ã€‚

å°†GRUçš„è¾“å…¥ï¼ˆrnn_inputï¼‰å’Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆprev_h_batchï¼‰ä½œä¸ºè¾“å…¥ä¼ å…¥GRUå±‚ï¼ˆself.rnnï¼‰ã€‚GRUçš„è¾“å‡ºåŒ…æ‹¬æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€è¾“å‡ºï¼ˆdec_rnn_outputï¼‰å’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆdec_hiddenï¼‰ã€‚

æœ€åï¼Œå°†GRUçš„è¾“å‡ºï¼ˆdec_rnn_outputï¼‰é€šè¿‡çº¿æ€§å±‚ï¼ˆself.W_outï¼‰è¿›è¡Œå˜æ¢ï¼Œå¹¶åº”ç”¨å¯¹æ•°softmaxå‡½æ•°ï¼ˆself.log_softmaxï¼‰å¾—åˆ°è§£ç å™¨çš„è¾“å‡ºï¼ˆdec_outputï¼‰ã€‚

```python
    def forward(self, prev_y_batch, prev_h_batch, encoder_outputs_batch):
        attn_weights = self.attn(prev_h_batch, encoder_outputs_batch)
        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs_batch.transpose(0, 1))
        y_ctx = torch.cat((prev_y_batch, context.squeeze(1)), 1)       
        rnn_input = self.W_combine(y_ctx)
        dec_rnn_output, dec_hidden = self.rnn(rnn_input.unsqueeze(0), prev_h_batch)
        unnormalized_logits = self.W_out(dec_rnn_output[0])
        dec_output = self.log_softmax(unnormalized_logits)
        return dec_output, dec_hidden, attn_weights
```



è¿™ä¸ªè§£ç å™¨ç±»ç”¨äºå®ç°è§£ç å™¨çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œå°†è¾“å…¥çš„å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºå’Œéšè—çŠ¶æ€ä¸ç¼–ç å™¨çš„è¾“å‡ºè¿›è¡Œå¤„ç†ï¼Œå¾—åˆ°è§£ç å™¨çš„è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡ã€‚



### 3.3 Attention

Attentionæ¨¡å—ç”¨äºåœ¨è§£ç å™¨ä¸­è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä»¥ä¾¿å°†ç¼–ç å™¨çš„ä¸åŒéƒ¨åˆ†çš„ä¿¡æ¯èšç„¦åˆ°è§£ç å™¨çš„å½“å‰æ­¥éª¤ä¸Šã€‚

åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­ï¼ŒAttentionæ¨¡å—æ¥æ”¶ç¼–ç å™¨éšè—å±‚è¾“å‡ºç»´åº¦ï¼ˆencoder_hidden_dimï¼‰å’Œè§£ç å™¨éšè—çŠ¶æ€çš„ç»´åº¦ï¼ˆdecoder_hidden_dimï¼‰ã€‚å®ƒè¿˜å¯ä»¥é€‰æ‹©æ€§åœ°æ¥æ”¶æ³¨æ„åŠ›æƒé‡çš„ç»´åº¦ï¼ˆattn_dimï¼‰ã€‚

åœ¨å‰å‘ä¼ æ’­æ–¹æ³•ä¸­ï¼ŒAttentionæ¨¡å—æ¥æ”¶è§£ç å™¨çš„å…ˆå‰éšè—çŠ¶æ€ï¼ˆprev_h_batchï¼‰å’Œç¼–ç å™¨çš„è¾“å‡ºï¼ˆenc_outputsï¼‰ã€‚è¿™é‡Œçš„enc_outputsæ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º[seq_len, batch_size, encoder_hidden_dim]ï¼Œå…¶ä¸­seq_lenè¡¨ç¤ºç¼–ç å™¨è¾“å‡ºçš„åºåˆ—é•¿åº¦ï¼Œbatch_sizeè¡¨ç¤ºæ‰¹é‡å¤§å°ï¼Œencoder_hidden_dimè¡¨ç¤ºç¼–ç å™¨éšè—å±‚è¾“å‡ºçš„ç»´åº¦ã€‚

é¦–å…ˆï¼Œé€šè¿‡çº¿æ€§å˜æ¢Uï¼Œå°†ç¼–ç å™¨çš„è¾“å‡ºenc_outputsè¿›è¡Œå˜æ¢ï¼Œä½¿å…¶ç»´åº¦å˜ä¸º[self.h_dim * self.num_directions, self.a_dim]ï¼Œå…¶ä¸­self.h_dimè¡¨ç¤ºç¼–ç å™¨éšè—å±‚è¾“å‡ºçš„ç»´åº¦ï¼Œself.num_directionsä¸º1ï¼ˆå› ä¸ºä¸è€ƒè™‘åŒå‘ç¼–ç å™¨ï¼‰ï¼Œself.a_dimè¡¨ç¤ºæ³¨æ„åŠ›æƒé‡çš„ç»´åº¦ã€‚

ç„¶åï¼Œé€šè¿‡çº¿æ€§å˜æ¢Wï¼Œå°†è§£ç å™¨çš„å…ˆå‰éšè—çŠ¶æ€prev_h_batchè¿›è¡Œå˜æ¢ï¼Œä½¿å…¶ç»´åº¦å˜ä¸º[self.s_dim, self.a_dim]ï¼Œå…¶ä¸­self.s_dimè¡¨ç¤ºè§£ç å™¨éšè—çŠ¶æ€çš„ç»´åº¦ã€‚

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨unsqueeze(0)å°†å˜æ¢åçš„è§£ç å™¨éšè—çŠ¶æ€å˜ä¸ºä¸‰ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º[1, batch_size, self.a_dim]ï¼Œä»¥ä¾¿ä¸ç¼–ç å™¨è¾“å‡ºuhè¿›è¡Œç›¸åŠ ã€‚

å°†å˜æ¢åçš„è§£ç å™¨éšè—çŠ¶æ€è¿›è¡Œæ‰©å±•ï¼Œä½¿å…¶ä¸ç¼–ç å™¨è¾“å‡ºuhçš„å½¢çŠ¶ç›¸åŒã€‚

å°†æ‰©å±•åçš„è§£ç å™¨éšè—çŠ¶æ€å’Œç¼–ç å™¨è¾“å‡ºè¿›è¡Œå…ƒç´ çº§ç›¸åŠ ï¼Œå¹¶é€šè¿‡tanhå‡½æ•°è¿›è¡Œæ¿€æ´»ï¼Œå¾—åˆ°wquhã€‚

é€šè¿‡çº¿æ€§å˜æ¢vï¼Œå°†wquhè¿›è¡Œå˜æ¢ï¼Œä½¿å…¶ç»´åº¦å˜ä¸º[batch_size, src_seq_len]ï¼Œå…¶ä¸­src_seq_lenè¡¨ç¤ºç¼–ç å™¨è¾“å‡ºçš„åºåˆ—é•¿åº¦ã€‚

æœ€åï¼Œé€šè¿‡softmaxå‡½æ•°å¯¹å¾—åˆ°çš„æ³¨æ„åŠ›å¾—åˆ†è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡attn_weightsã€‚attn_weightsçš„å½¢çŠ¶ä¸º[batch_size, src_seq_len]ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬åœ¨ç¼–ç å™¨è¾“å‡ºçš„æ¯ä¸ªä½ç½®ä¸Šçš„æ³¨æ„åŠ›æƒé‡ã€‚

æ³¨æ„åŠ›æƒé‡attn_weightsè¢«è¿”å›ï¼Œä»¥ä¾¿åœ¨è§£ç å™¨çš„åç»­æ­¥éª¤ä¸­ä½¿ç”¨ã€‚

```python

class Attention(nn.Module):
    def __init__(self, encoder_hidden_dim, decoder_hidden_dim, attn_dim=None):
        super(Attention, self).__init__()
        self.num_directions = 1     # ä¸è€ƒè™‘åŒå‘ç¼–ç å™¨
        self.h_dim = encoder_hidden_dim
        self.s_dim = decoder_hidden_dim
        self.a_dim = self.s_dim if attn_dim is None else attn_dim
        # æ„å»ºæ³¨æ„åŠ›
        self.U = nn.Linear(self.h_dim * self.num_directions, self.a_dim)
        self.W = nn.Linear(self.s_dim, self.a_dim)
        self.v = nn.Linear(self.a_dim, 1)
        self.tanh = nn.Tanh()
        self.softmax = nn.Softmax()

    def forward(self, prev_h_batch, enc_outputs):
        src_seq_len,batch_size,enc_dim = enc_outputs.size()
        uh = self.U(enc_outputs.view(-1, self.h_dim)).
        		view(src_seq_len,batch_size, self.a_dim)  
        wq = self.W(prev_h_batch.view(-1, self.s_dim)).unsqueeze(0)  
        wq3d = wq.expand_as(uh)
        wquh = self.tanh(wq3d + uh)
        attn_unnorm_scores = self.v(wquh.view(-1, self.a_dim)).
        		view(batch_size, src_seq_len)
        attn_weights = self.softmax(attn_unnorm_scores)  
        return attn_weights
```





### 3.4 Seq2Seq

Seq2Seqæ¨¡å‹çš„å®šä¹‰ï¼Œå®ƒåŒ…å«äº†ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ã€‚

åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­ï¼ŒSeq2Seqæ¨¡å‹æ¥æ”¶äº†ä¸€äº›é…ç½®ä¿¡æ¯ï¼ˆconfigï¼‰ã€è®¾å¤‡ä¿¡æ¯ï¼ˆdeviceï¼‰ã€æºè¯­è¨€è¯è¡¨å¤§å°ï¼ˆsrc_vocab_sizeï¼‰å’Œç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°ï¼ˆtgt_vocab_sizeï¼‰ã€‚

```python
class Seq2Seq(nn.Module):
    
    def __init__(self, config, device, src_vocab_size, tgt_vocab_size):
        super(Seq2Seq, self).__init__()
        self.device = device  # è®¾å¤‡
        self.config = config
        self.src_vocab_size = src_vocab_size
        self.tgt_vocab_size = tgt_vocab_size
        # æ„å»ºè¯åµŒå…¥å±‚
        self.embedding_mat = nn.Embedding(src_vocab_size, config.embedding_dimension, padding_idx=PAD_ID)
        self.embedding_dropout_layer = nn.Dropout(config.dropout)
        # æ„å»ºç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder = Encoder(input_size=config.encoder_input_size,
                               hidden_size=config.encoder_hidden_size)

        self.decoder = Decoder(input_size=config.decoder_input_size,
                               hidden_size=config.decoder_hidden_size,
                               output_size=tgt_vocab_size,
                               embedding_dim=config.embedding_dimension,
                               encoder_hidden_size=config.encoder_hidden_size)
```

åœ¨å‰å‘ä¼ æ’­æ–¹æ³•ä¸­ï¼ŒSeq2Seqæ¨¡å‹æ¥æ”¶è¾“å…¥æ•°æ®ï¼ˆdataï¼‰ï¼Œå…¶ä¸­åŒ…å«æºè¯­è¨€æ•°æ®å’Œç›®æ ‡è¯­è¨€æ•°æ®ï¼ˆbatch_x_varå’Œbatch_y_varï¼‰ã€‚é¦–å…ˆï¼Œå°†æºè¯­è¨€æ•°æ®é€šè¿‡è¯åµŒå…¥å±‚ï¼ˆembedding_matï¼‰è¿›è¡Œè¯åµŒå…¥å¾—åˆ°ç¼–ç å™¨çš„è¾“å…¥ï¼ˆencoder_input_embeddedï¼‰ã€‚ç„¶åï¼Œå°†ç¼–ç å™¨çš„è¾“å…¥ä¼ é€’ç»™ç¼–ç å™¨ï¼ˆencoderï¼‰è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°ç¼–ç å™¨çš„è¾“å‡ºï¼ˆencoder_outputsï¼‰å’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆencoder_hiddenï¼‰ã€‚

æ¥ä¸‹æ¥ï¼Œæ ¹æ®ç›®æ ‡è¯­è¨€æ•°æ®çš„é•¿åº¦å’Œæ‰¹é‡å¤§å°ï¼Œåˆå§‹åŒ–è§£ç å™¨çš„éšè—çŠ¶æ€ï¼ˆdec_hiddenï¼‰å’Œè§£ç å™¨çš„è¾“å…¥ï¼ˆdec_inputï¼‰ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªå¾ªç¯ï¼Œä¾æ¬¡è§£ç ç›®æ ‡è¯­è¨€çš„æ¯ä¸ªè¯ã€‚åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­ï¼Œå°†ä¸Šä¸€ä¸ªè¾“å‡ºçš„è¯åµŒå…¥ï¼ˆprev_yï¼‰ä¼ é€’ç»™è§£ç å™¨ï¼ˆdecoderï¼‰è¿›è¡Œè§£ç ï¼Œå¾—åˆ°è§£ç å™¨çš„è¾“å‡ºï¼ˆdec_outputï¼‰ã€æ›´æ–°åçš„è§£ç å™¨éšè—çŠ¶æ€ï¼ˆdec_hiddenï¼‰å’Œæ³¨æ„åŠ›æƒé‡ï¼ˆattn_weightsï¼‰ã€‚å°†è§£ç å™¨çš„è¾“å‡ºè®°å½•åˆ°logitsä¸­ï¼Œç”¨äºè®¡ç®—æŸå¤±å‡½æ•°ã€‚åŒæ—¶ï¼Œå°†è§£ç å™¨çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªè§£ç æ­¥éª¤çš„è¾“å…¥ï¼ˆdec_inputï¼‰ï¼Œç»§ç»­è¿›è¡Œè§£ç ã€‚

å…·ä½“æ¥è¯´ï¼Œåœ¨å‰å‘ä¼ æ’­æ–¹æ³•ä¸­ï¼š

1. é¦–å…ˆï¼Œè¾“å…¥æ•°æ®è¢«æ‹†åˆ†ä¸ºæºè¯­è¨€æ•°æ®ï¼ˆbatch_x_varï¼‰å’Œç›®æ ‡è¯­è¨€æ•°æ®ï¼ˆbatch_y_varï¼‰ã€‚
2. æºè¯­è¨€æ•°æ®é€šè¿‡è¯åµŒå…¥å±‚ï¼ˆembedding_matï¼‰è¿›è¡Œè¯åµŒå…¥æ“ä½œï¼Œå¾—åˆ°ç¼–ç å™¨çš„è¾“å…¥ï¼ˆencoder_input_embeddedï¼‰ã€‚è¯åµŒå…¥æ“ä½œå°†æ¯ä¸ªè¯çš„ç´¢å¼•è½¬æ¢ä¸ºä¸€ä¸ªè¯å‘é‡è¡¨ç¤ºã€‚
3. è¯åµŒå…¥åï¼Œç¼–ç å™¨ï¼ˆencoderï¼‰æ¥æ”¶ç¼–ç å™¨çš„è¾“å…¥ï¼Œè¿›è¡Œç¼–ç æ“ä½œã€‚ç¼–ç å™¨ä½¿ç”¨GRUå•å…ƒï¼Œå°†è¾“å…¥åºåˆ—é€æ­¥ç¼–ç ä¸ºä¸€ç³»åˆ—éšè—çŠ¶æ€ã€‚ç¼–ç å™¨è¾“å‡ºåŒ…æ‹¬ç¼–ç å™¨çš„è¾“å‡ºï¼ˆencoder_outputsï¼‰å’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆencoder_hiddenï¼‰ã€‚
4. è§£ç å™¨çš„åˆå§‹åŒ–ï¼š
   - è§£ç å™¨çš„éšè—çŠ¶æ€ï¼ˆdec_hiddenï¼‰ä½¿ç”¨ç¼–ç å™¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆencoder_hiddenï¼‰æ¥åˆå§‹åŒ–ã€‚
   - è§£ç å™¨çš„è¾“å…¥ï¼ˆdec_inputï¼‰åˆå§‹åŒ–ä¸ºèµ·å§‹ç¬¦å·çš„ç´¢å¼•ï¼ˆBOS_IDï¼‰ã€‚
5. å¾ªç¯è§£ç ï¼š
   - åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­ï¼Œé¦–å…ˆå°†è§£ç å™¨çš„è¾“å…¥è¯åµŒå…¥ï¼ˆprev_yï¼‰ä¼ é€’ç»™è§£ç å™¨ï¼ˆdecoderï¼‰è¿›è¡Œè§£ç æ“ä½œã€‚è§£ç å™¨ä½¿ç”¨GRUå•å…ƒï¼Œç»“åˆä¸Šä¸€ä¸ªè¾“å‡ºçš„è¯åµŒå…¥å’Œè§£ç å™¨çš„éšè—çŠ¶æ€ï¼Œç”Ÿæˆå½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºã€‚
   - è§£ç å™¨çš„è¾“å‡ºç»è¿‡çº¿æ€§å˜æ¢ï¼ˆself.W_outï¼‰å’Œå¯¹æ•°softmaxå‡½æ•°ï¼ˆself.log_softmaxï¼‰ï¼Œå¾—åˆ°å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼ˆdec_outputï¼‰ã€‚
   - è§£ç å™¨çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒè®°å½•åœ¨logitsä¸­ï¼Œç”¨äºè®¡ç®—æŸå¤±å‡½æ•°ã€‚
   - å½“å‰æ—¶é—´æ­¥çš„ç›®æ ‡è¯ï¼ˆbatch_y_var[di]ï¼‰ä½œä¸ºä¸‹ä¸€ä¸ªè§£ç æ­¥éª¤çš„è¾“å…¥ï¼ˆdec_inputï¼‰ï¼Œç»§ç»­è§£ç ã€‚
6. è¿”å›logitsä½œä¸ºæ¨¡å‹çš„è¾“å‡ºã€‚



```python
def forward(self, data):
        """
        Args:
            data(tuple): (source, target)
        Returns:
            [seq_len, batch_size, vocab_size]
        """
        batch_x_var, batch_y_var = data     # [seq_len, batch_size] * 2
        # è¯åµŒå…¥
        # [seq_len, batch_size, embed_dim]
        encoder_input_embedded = self.embedding_mat(batch_x_var)
        encoder_input_embedded = self.embedding_dropout_layer(encoder_input_embedded)
        # Encode
        # [batch_size, seq_len, embed_size], [1,batch_size,embed_size]
        encoder_outputs, encoder_hidden = self.encoder(encoder_input_embedded)
        # Decode
        dec_len, batch_size = batch_y_var.size()[0], batch_y_var.size()[1]
        # å½“å®ç°è§£ç å™¨æ—¶ï¼Œç›´æ¥ä½¿ç”¨ç¼–ç å™¨æœ€åâ¼€ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€æ¥åˆå§‹åŒ–è§£ç å™¨çš„éšçŠ¶æ€ã€‚
        dec_hidden = encoder_hidden
       
        dec_input = Variable(torch.LongTensor([BOS_ID] * batch_size)).to(self.device)

        logits = Variable(torch.zeros(dec_len, batch_size, self.tgt_vocab_size)).to(self.device)

        for di in range(dec_len):
            # ä¸Šä¸€ä¸ªè¾“å‡ºçš„è¯åµŒå…¥
            prev_y = self.embedding_mat(dec_input)      # [seq_len?batch_size,embed_dim]
            dec_output, dec_hidden, attn_weights = self.decoder(prev_y, dec_hidden, encoder_outputs)
            logits[di] = dec_output  # è®°å½•è¾“å‡ºè¯çš„æ¦‚ç‡
            dec_input = batch_y_var[di]

        return logits
```

åœ¨é¢„æµ‹æ–¹æ³•ä¸­ï¼Œæ ¹æ®è¾“å…¥çŸ©é˜µï¼ˆsource_tensorï¼‰è¿›è¡Œé¢„æµ‹è¾“å‡ºã€‚é¦–å…ˆï¼Œå°†è¾“å…¥çŸ©é˜µè¿›è¡Œè¯åµŒå…¥å¾—åˆ°ç¼–ç å™¨çš„è¾“å…¥ã€‚ç„¶åï¼Œå°†ç¼–ç å™¨çš„è¾“å…¥ä¼ é€’ç»™ç¼–ç å™¨è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°ç¼–ç å™¨çš„è¾“å‡ºå’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚æ¥ä¸‹æ¥ï¼Œåˆå§‹åŒ–è§£ç å™¨çš„è¾“å…¥ä¸ºèµ·å§‹ç¬¦å·ï¼Œå¹¶å°†èµ·å§‹ç¬¦å·çš„è¯åµŒå…¥ï¼ˆprev_yï¼‰ä¼ é€’ç»™è§£ç å™¨è¿›è¡Œè§£ç ï¼Œå¾—åˆ°è§£ç å™¨çš„è¾“å‡ºã€æ›´æ–°åçš„è§£ç å™¨éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æƒé‡ã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°é‡åˆ°ç»ˆæ­¢ç¬¦å·æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ã€‚åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­ï¼Œè®°å½•è§£ç ç»“æœå’Œæ³¨æ„åŠ›æƒé‡ï¼Œå¹¶å°†è§£ç å™¨çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªè§£ç æ­¥éª¤çš„è¾“å…¥ã€‚

æ›´å…·ä½“æ¥è¯´ï¼š

1. é¦–å…ˆï¼Œå°†è¾“å…¥çŸ©é˜µï¼ˆsource_tensorï¼‰è¿›è¡Œè¯åµŒå…¥æ“ä½œï¼Œå¾—åˆ°ç¼–ç å™¨çš„è¾“å…¥ã€‚
2. å°†ç¼–ç å™¨çš„è¾“å…¥ä¼ é€’ç»™ç¼–ç å™¨è¿›è¡Œç¼–ç æ“ä½œï¼Œå¾—åˆ°ç¼–ç å™¨çš„è¾“å‡ºå’Œæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚
3. åˆå§‹åŒ–è§£ç å™¨çš„è¾“å…¥ä¸ºèµ·å§‹ç¬¦å·çš„ç´¢å¼•ï¼ˆBOS_IDï¼‰ï¼Œå¹¶å°†èµ·å§‹ç¬¦å·çš„è¯åµŒå…¥ä¼ é€’ç»™è§£ç å™¨è¿›è¡Œè§£ç æ“ä½œã€‚
4. å¾ªç¯è§£ç ï¼š
   - åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­ï¼Œè§£ç å™¨æ¥æ”¶ä¸Šä¸€ä¸ªè¾“å‡ºçš„è¯åµŒå…¥ï¼ˆprev_yï¼‰å’Œè§£ç å™¨çš„éšè—çŠ¶æ€ï¼ˆdec_hiddenï¼‰ï¼Œç”Ÿæˆå½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºã€‚
   - è®°å½•å½“å‰æ—¶é—´æ­¥çš„è§£ç ç»“æœï¼ˆdecoded_idsï¼‰å’Œæ³¨æ„åŠ›æƒé‡ï¼ˆattn_wï¼‰ã€‚
   - æ ¹æ®å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œé€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„è¯ä½œä¸ºå½“å‰æ—¶é—´æ­¥çš„é¢„æµ‹ç»“æœã€‚
   - å°†å½“å‰é¢„æµ‹ç»“æœä½œä¸ºä¸‹ä¸€ä¸ªè§£ç æ­¥éª¤çš„è¾“å…¥ï¼Œå¹¶æ›´æ–°è§£ç æ­¥éª¤çš„ç´¢å¼•ï¼ˆcurr_dec_idxï¼‰ã€‚
     1. ç»§ç»­å¾ªç¯è§£ç ï¼Œç›´åˆ°é‡åˆ°ç»ˆæ­¢ç¬¦å·ï¼ˆEOS_IDï¼‰æˆ–è¾¾åˆ°æœ€å¤§å¥å­é•¿åº¦ï¼ˆconfig.max_sentence_lengthï¼‰ä¸ºæ­¢ã€‚
     2. è¿”å›è§£ç ç»“æœï¼ˆdecoded_idsï¼‰å’Œæ³¨æ„åŠ›æƒé‡ï¼ˆattn_wï¼‰ä½œä¸ºé¢„æµ‹æ–¹æ³•çš„è¾“å‡ºã€‚

è¿™æ ·ï¼ŒSeq2Seqæ¨¡å‹çš„å‰å‘ä¼ æ’­æ–¹æ³•å¯ä»¥å°†æºè¯­è¨€æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆç›®æ ‡è¯­è¨€æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒã€‚é¢„æµ‹æ–¹æ³•å¯ä»¥æ ¹æ®è¾“å…¥çŸ©é˜µé¢„æµ‹è¾“å‡ºåºåˆ—ã€‚



```python

    def predict(self, source_tensor):
        encoder_input_embedded = self.embedding_mat(source_tensor)
        encoder_outputs, encoder_hidden = self.encoder(encoder_input_embedded)

        decoded_ids, attn_w = [], []
        curr_token_id = BOS_ID
        curr_dec_idx = 0
        dec_input_var = Variable(torch.LongTensor([curr_token_id]))

        dec_input_var = dec_input_var.to(self.device)
        dec_hidden = encoder_hidden[:1] 
        # ç›´åˆ° EOS æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦
        while curr_token_id != EOS_ID and curr_dec_idx <= self.config.max_sentence_length:
            prev_y = self.embedding_mat(dec_input_var)  
            decoder_output, dec_hidden, decoder_attention = self.decoder(prev_y, dec_hidden, encoder_outputs)
            attn_w.append(decoder_attention.data.cpu().numpy().tolist()[0])
            topval, topidx = decoder_output.data.topk(1)  
            curr_token_id = topidx[0][0]
            decoded_ids.append(int(curr_token_id.cpu().numpy()))
            dec_input_var = (Variable(torch.LongTensor([curr_token_id]))).to(self.device)
            curr_dec_idx += 1
        return decoded_ids, attn_w
```





## 4. è¯„ä»·æŒ‡æ ‡BLEU

Papineni K, Roukos S, Ward T, et al. Bleu: a method for automatic evaluation of machine translation[C]//Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002: 311-318.



BLEUï¼ˆBilingual Evaluation Understudyï¼‰æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œè¯„ä¼°ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚BLEU-4æ˜¯BLEUæŒ‡æ ‡çš„ä¸€ç§å˜ä½“ï¼Œç‰¹åˆ«å…³æ³¨4-gramç²¾ç¡®åº¦ã€‚

BLEU-4ä¸­æœ‰å¦‚ä¸‹çš„å‡ ä¸ªæ¦‚å¿µï¼š

* **N-gramåŒ¹é…**ï¼šæœºå™¨ç”Ÿæˆçš„è¾“å‡ºå’Œå‚è€ƒç¿»è¯‘è¢«åˆ†å‰²æˆn-gramï¼ˆè¿ç»­çš„nä¸ªè¯ï¼‰ç‰‡æ®µã€‚ä¾‹å¦‚ï¼Œ1-gramç”±å•ä¸ªå•è¯ç»„æˆï¼Œ2-gramç”±ç›¸é‚»çš„è¯å¯¹ç»„æˆï¼Œ3-gramç”±ä¸‰ä¸ªè¯ç»„æˆï¼Œä¾æ­¤ç±»æ¨ã€‚
* **N-gramè®¡æ•°**ï¼šè®¡ç®—æœºå™¨ç”Ÿæˆçš„è¾“å‡ºä¸­æ¯ä¸ªn-gramçš„å‡ºç°æ¬¡æ•°ã€‚

* **N-gramç²¾ç¡®åº¦è®¡ç®—**ï¼šBLEU-4è®¡ç®—æœºå™¨ç”Ÿæˆçš„è¾“å‡ºä¸å‚è€ƒï¼ˆäººå·¥ç”Ÿæˆçš„ï¼‰ç¿»è¯‘ä¹‹é—´1-gramã€2-gramã€3-gramå’Œ4-gramçš„åŒ¹é…ç²¾ç¡®åº¦ã€‚ç²¾ç¡®åº¦è¡¡é‡æœºå™¨ç”Ÿæˆçš„è¾“å‡ºä¸­æœ‰å¤šå°‘ä¸ªn-gramä¸å‚è€ƒç¿»è¯‘å®Œå…¨åŒ¹é…ã€‚

* **ç®€æ´åº¦æƒ©ç½š**ï¼šBLEU-4å¼•å…¥äº†ç®€æ´åº¦æƒ©ç½šï¼Œä»¥åº”å¯¹æœºå™¨ç”Ÿæˆçš„è¾“å‡ºæ˜æ˜¾çŸ­äºå‚è€ƒç¿»è¯‘çš„æƒ…å†µã€‚ç®€æ´åº¦æƒ©ç½šæ˜¯ä¸€ä¸ªå› å­ï¼Œå¦‚æœæœºå™¨ç”Ÿæˆçš„è¾“å‡ºè¾ƒçŸ­ï¼Œåˆ™ä¼šé™ä½BLEUåˆ†æ•°ã€‚å®ƒé¼“åŠ±ç”Ÿæˆä¸å‚è€ƒç¿»è¯‘é•¿åº¦æ›´æ¥è¿‘çš„ç¿»è¯‘ç»“æœã€‚ç®€æ´æƒ©ç½šåº¦ç”±BPè¡¨ç¤ºï¼ŒBPçš„å®šä¹‰ä¸ºï¼š

$$
BP=\begin{cases}1\quad if \quad c>r\\e^{1-r/c}\quad if\quad c\le r\end{cases}
$$

â€‹	å…¶ä¸­ğ‘ä¸ºå€™é€‰æ–‡æœ¬çš„é•¿åº¦ï¼Œğ‘Ÿ ä¸ºä¸å€™é€‰æ–‡æœ¬é•¿åº¦æœ€è¿‘æ¥çš„å‚è€ƒæ–‡æœ¬çš„é•¿

* **ç»¼åˆç²¾ç¡®åº¦**ï¼šå°†å„ä¸ªn-gramç²¾ç¡®åº¦é€šè¿‡åŠ æƒå‡ ä½•å¹³å‡è¿›è¡Œåˆå¹¶ã€‚é€šå¸¸ä¸ºæ¯ä¸ªn-gramç²¾ç¡®åº¦è®¾ç½®æƒé‡ä¸º1/4ã€‚

* **æœ€ç»ˆBLEUåˆ†æ•°**ï¼šå°†ç»¼åˆç²¾ç¡®åº¦ä¹˜ä»¥ç®€æ´åº¦æƒ©ç½šï¼Œå¾—åˆ°æœ€ç»ˆçš„BLEU-4åˆ†æ•°ã€‚ç®€æ´åº¦æƒ©ç½šæœ‰åŠ©äºæƒ©ç½šè¿‡çŸ­çš„ç¿»è¯‘ç»“æœã€‚

BLEU-4åˆ†æ•°çš„èŒƒå›´åœ¨0åˆ°1ä¹‹é—´ï¼Œ1è¡¨ç¤ºæœºå™¨ç”Ÿæˆçš„è¾“å‡ºä¸å‚è€ƒç¿»è¯‘åœ¨4-gramç²¾ç¡®åº¦æ–¹é¢å®Œå…¨åŒ¹é…ã€‚BLEUçš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
$$
BLEU=BP\cdot exp(\sum_{n=1}^{N}w_nlogp_n)
$$
â€‹	å…¶ä¸­$w_n$ä¸ºn-gramçš„æƒé‡ï¼ŒåŸæ–‡ä¸­æè¿°çš„$w_n=1/N$ï¼Œ$p_n $ä¸ºå€™é€‰æ–‡æœ¬n-gramçš„å¾—åˆ†ã€‚



æ­¤éƒ¨åˆ†å®ç°è®¡ç®—BLEUåˆ†æ•°çš„ç±»`BLEUScore`ã€‚

é¦–å…ˆè®¾ç½®åˆå§‹åŒ–å‡½æ•°`__init__`ã€‚åˆå§‹åŒ–BLEUScoreå¯¹è±¡æ—¶ï¼Œå¯ä»¥æŒ‡å®šæœ€å¤§çš„n-gramå¤§å°ï¼ˆé»˜è®¤ä¸º4ï¼‰å’Œæ˜¯å¦åŒºåˆ†å¤§å°å†™ï¼ˆé»˜è®¤ä¸ºFalseï¼‰ã€‚

```python
def __init__(self, max_ngram=4, case_sensitive=False):
        self.max_ngram = max_ngram
        self.case_sensitive = case_sensitive
        self.hits = [0] * self.max_ngram
        self.cand_lens = [0] * self.max_ngram
        self.ref_len = 0
        self.reset()
```



å®šä¹‰`reset`æ–¹æ³•ç”¨äºé‡ç½®è®¡æ•°å™¨ï¼Œå°†å‘½ä¸­æ¬¡æ•°ã€é¢„æµ‹é•¿åº¦å’Œå‚è€ƒé•¿åº¦éƒ½é‡ç½®ä¸º0ã€‚

```python
def reset(self):
        self.hits = [0] * self.max_ngram
        self.cand_lens = [0] * self.max_ngram
        self.ref_len = 0
```



å®šä¹‰åˆ†è¯å‡½æ•°`tokenize`ï¼Œè¯¥æ–¹æ³•ç”¨äºå¯¹è¾“å…¥çš„å¥å­è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œä¸»è¦æ˜¯å»é™¤ä¸€äº›æ ‡ç‚¹ç¬¦å·å¹¶å°†è¯æŒ‰ç©ºæ ¼åˆ†å¼€ã€‚

```python
def tokenize(self, sentence):
        """å¯¹è¾“å…¥çš„å¥å­è¿›è¡Œåˆ†è¯ï¼Œä¸»è¦æ˜¯å»é™¤ä¸€äº›æ ‡ç‚¹ç¬¦å·å¹¶å°†è¯æŒ‰ç©ºæ ¼åˆ†å¼€"""
        sentence = re.sub(r'[^\w\s]', '', sentence)
        return sentence.split()
```



å®šä¹‰`append`æ–¹æ³•ï¼Œç”¨äºå°†é¢„æµ‹å¥å­å’Œå‚è€ƒå¥å­æ·»åŠ åˆ°è®¡ç®—ä¸­ã€‚é¦–å…ˆå¯¹é¢„æµ‹å¥å­å’Œå‚è€ƒå¥å­è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œç„¶åæ ¹æ®n-gramçš„å¤§å°è®¡ç®—å‘½ä¸­æ¬¡æ•°å’Œé¢„æµ‹é•¿åº¦ã€‚é€‰æ‹©ä¸é¢„æµ‹å¥å­é•¿åº¦æœ€æ¥è¿‘çš„å‚è€ƒå¥å­ï¼Œå¹¶è®°å½•å‚è€ƒå¥å­çš„é•¿åº¦ã€‚

```python
def append(self, predicted_sentence, ref_sentences):
        predicted_sentence = predicted_sentence if 
        				isinstance(predicted_sentence, list) else 
                        self.tokenize(predicted_sentence)
        ref_sentences = [ref_sent if isinstance(ref_sent, list) else
                         self.tokenize(ref_sent) for ref_sent in ref_sentences]
        for i in range(self.max_ngram):
            # è®¡ç®—æ¯ä¸ª gram çš„å‘½ä¸­æ¬¡æ•°
            self.hits[i] += self.compute_hits(i + 1, 
                                              predicted_sentence, ref_sentences)
            # è®¡ç®—æ¯ä¸ª gram çš„é¢„æµ‹é•¿åº¦
            self.cand_lens[i] += len(predicted_sentence) - i
        # é€‰æ‹©é•¿åº¦æœ€ç›¸è¿‘çš„å‚è€ƒæ–‡æœ¬
        closest_ref = min(ref_sentences, 
                          key=lambda ref_sent: 
                          (abs(len(ref_sent) - len(predicted_sentence)),
                           len(ref_sent)))
        # è®°å½•å‚è€ƒæ–‡æœ¬é•¿åº¦
        self.ref_len += len(closest_ref)
```



å®šä¹‰`compute_hits`æ–¹æ³•ï¼Œç”¨äºè®¡ç®—ç»™å®šn-gramå¤§å°çš„å‘½ä¸­æ¬¡æ•°ã€‚é¦–å…ˆå°†å‚è€ƒå¥å­è¿›è¡Œn-gramçš„ç»Ÿè®¡ï¼Œç„¶åå¯¹é¢„æµ‹å¥å­è¿›è¡Œn-gramçš„ç»Ÿè®¡ï¼Œè®¡ç®—é¢„æµ‹å¥å­ä¸­å‘½ä¸­çš„n-gramä¸ªæ•°ã€‚

```python
 def compute_hits(self, n, predicted_sentence, ref_sentences):
        merged_ref_ngrams = self.get_ngram_counts(n, ref_sentences)
        pred_ngrams = self.get_ngram_counts(n, [predicted_sentence])
        hits = 0
        for ngram, cnt in pred_ngrams.items():
            hits += min(merged_ref_ngrams.get(ngram, 0), cnt)
        return hits
```



å®šä¹‰`get_ngram_counts`æ–¹æ³•ï¼Œç”¨äºè·å–ç»™å®šn-gramå¤§å°çš„ç»Ÿè®¡ä¿¡æ¯ã€‚é¦–å…ˆå°†å¥å­æŒ‰ç…§n-gramå¤§å°èšåˆï¼Œç„¶åç»Ÿè®¡æ¯ä¸ªn-gramçš„å‡ºç°æ¬¡æ•°ï¼Œå¹¶å–æœ€å¤§å€¼ã€‚

```python
def get_ngram_counts(self, n, sentences):
        merged_ngrams = {}
        # æŒ‰ gram æ•°èšåˆå¥å­
        for sent in sentences:
            ngrams = defaultdict(int)
            if not self.case_sensitive:
                ngrams_list = list(zip(*[[tok.lower() for tok in sent[i:]] for i in range(n)]))
            else:
                ngrams_list = list(zip(*[sent[i:] for i in range(n)]))
            for ngram in ngrams_list:
                ngrams[ngram] += 1
            for ngram, cnt in ngrams.items():
                merged_ngrams[ngram] = max((merged_ngrams.get(ngram, 0), cnt))
        return merged_ngrams
```



æœ€åå®šä¹‰`score`æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨äºè®¡ç®—æœ€ç»ˆçš„BLEUåˆ†æ•°ã€‚é¦–å…ˆè®¡ç®—çŸ­å¥æƒ©ç½šå› å­ï¼ˆbpï¼‰ï¼Œæ ¹æ®é¢„æµ‹é•¿åº¦å’Œå‚è€ƒé•¿åº¦çš„æ¯”ä¾‹æ¥ç¡®å®šæƒ©ç½šå› å­çš„å¤§å°ã€‚ç„¶åè®¡ç®—æ¯ä¸ªn-gramçš„ç²¾ç¡®åº¦ï¼Œå¹¶ç´¯åŠ å…¶å¯¹æ•°å€¼ã€‚æœ€ç»ˆå¾—åˆ°BLEUåˆ†æ•°ã€‚

```python 
    def score(self):
        bp = 1.0
        # c <= r : BP=e^(1-r/c)
        # c > r : BP=1.0
        if self.cand_lens[0] <= self.ref_len:
            bp = math.exp(1.0 - self.ref_len / (float(self.cand_lens[0])
                                                if self.cand_lens[0] else 1e-5))
        prec_log_sum = 0.0
        for n_hits, n_len in zip(self.hits, self.cand_lens):
            n_hits = max(float(n_hits), self.TINY)

            n_len = max(float(n_len), self.SMALL)
            # è®¡ç®—âˆ‘logPn=âˆ‘log(n_hits/n_len)
            prec_log_sum += math.log(n_hits / n_len)
        return bp * math.exp((1.0 / self.max_ngram) * prec_log_sum)

```



## 5. è®­ç»ƒä¸éªŒè¯å‡½æ•°

### 5.1 è®­ç»ƒå‡½æ•°

æœ€åå®šä¹‰è®­ç»ƒå‡½æ•°ä¸æµ‹è¯•å‡½æ•°ã€‚è®­ç»ƒå‡½æ•°çš„å®šä¹‰å¦‚ä¸‹ï¼Œå…¥å‚ä¸ºDataloaderåŠ è½½çš„å¯¹è±¡ã€å½“å‰çš„epochæ•°å’Œæ€»å…±çš„epochæ•°ã€‚å°†æ•°æ®åŠ è½½åˆ°è¿›åº¦æ¡åº“tqdmæ–¹æ³•ä¸Šã€‚æ­¤å¤„éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒåŠ è½½çš„batch_dataä¸­çš„sourceå’Œtargetæ˜¯`[batch_size,seq_len]`å½¢çŠ¶çš„ï¼Œéœ€å¯¹å…¶è¿›è¡Œè½¬ç½®åä¼ å…¥æ¨¡å‹è¿›è¡Œè®¡ç®—ï¼Œä¹‹åè¿›è¡Œæ¢¯åº¦ä¸‹é™ç­‰è®­ç»ƒè¿‡ç¨‹ã€‚

```python
def train(data_loader, epoch_current, epoch_total):
    """æ¨¡å‹è®­ç»ƒå‡½æ•°"""
    model.train()
    total_loss = 0.0  # æ‰“å°è¾“å‡ºçš„loss
    t1 = time.time()
    with tqdm(total=len(data_loader),
              desc='Training epoch[{}/{}]'.format(epoch_current, epoch_total),
              file=sys.stdout) as t:
        for index, batch_data in enumerate(data_loader):
            source, target = batch_data
            source = source.to(device).transpose(0, 1)
			target = target.to(device).transpose(0, 1)
            optimizer.zero_grad()  # æ¢¯åº¦å€¼åˆå§‹åŒ–
            
            model_outputs = model((source, target))
            model_outputs = model_outputs.contiguous().view(-1, vocab_size)
            targets = target.contiguous().reshape(-1, 1).squeeze(1)
            
            loss = loss_function(model_outputs, targets.long())
            total_loss += loss.data.item()

            # æ¢¯åº¦ä¸‹é™
            loss.backward()
            optimizer.step()
            t.set_postfix(loss=total_loss / (index + 1), lr=scheduler.get_last_lr()[0], timecost=time.time()-t1)
            t.update(1)
        loss_list.append(total_loss / len(data_loader))
        lr_list.append(scheduler.get_last_lr()[0])
        scheduler.step()
```



### 5.2 éªŒè¯å‡½æ•°

éªŒè¯å‡½æ•°éœ€è¦å€ŸåŠ©`Seq2Seq`æ¨¡å‹ä¸­å®ç°çš„predictæ–¹æ³•ï¼Œé€ä¸ªå¯¹å¾…éªŒè¯çš„æ•°æ®é›†ä¸­çš„ç»“æ„åŒ–æ–‡æœ¬è¿›è¡Œç”Ÿæˆã€‚ä¹‹åè®¡ç®—åœ¨éªŒè¯é›†ä¸Šçš„BLEU-4å€¼ï¼Œè‹¥éªŒè¯æ•ˆæœæœ‰æ‰€æå‡ï¼Œå°±å°†å½“å‰æ¨¡å‹ä¿å­˜è‡³æœ¬åœ°ã€‚

```python
def validation(data_iterator, epoch_now):
    global best_bleu
    model.eval()
    sentences = []
    with torch.no_grad():
        for data in tqdm(data_iterator, desc="[Validation]{}".format(" "*(5+len(str(epoch_now)))), file=sys.stdout):
            src, tgt, lex, multi_target = data
            src = torch.as_tensor(src[:, np.newaxis]).to(device)
            sentence, attention = model.predict(src)
            # è§£ç å¥å­
            sentence = train_dataset.tokenizer.decode(sentence).replace('[NAME]', lex[0]).replace('[NEAR]', lex[1])
            sentences.append(sentence)
            scorer.append(sentence, multi_target)
        bleu = scorer.score()
        bleu_list.append(bleu)
        print("BLEU SCORE: {:.4f}".format(bleu))
        if bleu > best_bleu:
            state = {
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'bleu': bleu,
                'epoch': epoch_now,
            }
            if not os.path.exists(config.checkpoint_path):
                os.mkdir(config.checkpoint_path)
            torch.save(state, config.checkpoint_path + 'checkpoint.pth')
            print("æ¨¡å‹ä¿å­˜æˆåŠŸï¼ï¼")
            best_bleu = bleu
```





## 6. ä¸»ç¨‹åºä¸ç»“æœ

### 6.1 ä¸»ç¨‹åº

ä¸»ç¨‹åºçš„å®šä¹‰å¦‚ä¸‹ï¼Œå…¶ä¸­å®Œæˆäº†ä»¥ä¸‹å·¥ä½œï¼š

1. åˆ›å»º`BLEUScore`å®ä¾‹ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚
2. å®šä¹‰è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„è·¯å¾„ï¼Œå¹¶åˆ›å»ºç›¸åº”çš„`E2EDataset`å®ä¾‹ã€‚
3. åˆ›å»º`DataLoader`å®ä¾‹ï¼Œç”¨äºæŒ‰æ‰¹æ¬¡åŠ è½½æ•°æ®ã€‚
4. åˆå§‹åŒ–æ¨¡å‹ï¼Œå¹¶æ ¹æ®éœ€è¦åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹ã€‚
5. è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚è¿™é‡Œä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±å‡½æ•°å’Œéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¼˜åŒ–å™¨ã€‚
6. æ‰“å°æ¨¡å‹è®¾ç½®å’Œä»£ç è¿è¡Œç¯å¢ƒçš„ç›¸å…³ä¿¡æ¯ã€‚
7. è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯å¾ªç¯ã€‚é¦–å…ˆè¿›è¡Œè®­ç»ƒï¼Œç„¶åæ ¹æ®è®¾å®šçš„éªŒè¯é¢‘ç‡è¿›è¡ŒéªŒè¯ã€‚è®­ç»ƒå’ŒéªŒè¯çš„å…·ä½“å®ç°å¯èƒ½åœ¨åç»­çš„ä»£ç ä¸­å®šä¹‰ã€‚
8. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•æ¯ä¸ªepochçš„æŸå¤±å€¼ã€BLEUåˆ†æ•°å’Œå­¦ä¹ ç‡ç­‰ä¿¡æ¯ã€‚
9. åœ¨è®­ç»ƒå®Œæˆåï¼Œç»˜åˆ¶éªŒè¯BLEUåˆ†æ•°ã€è®­ç»ƒæŸå¤±å’Œå­¦ä¹ ç‡çš„æ›²çº¿å›¾ã€‚
10. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ã€‚

```python
if __name__ == "__main__":
    scorer = BLEUScore(max_ngram=4)
    trainSet_path = config.root_path + config.train_data_path
    devSet_path = config.root_path + config.dev_data_path
    testSet_path = config.root_path + config.test_data_path

    train_dataset = E2EDataset(trainSet_path, train_mod='train')

    dev_dataset = E2EDataset(devSet_path, train_mod='valid',
                             attributes_vocab=train_dataset.attributes_vocab,
                             tokenizer=train_dataset.tokenizer)

    test_dataset = E2EDataset(testSet_path, train_mod='test',
                              attributes_vocab=train_dataset.attributes_vocab,
                              tokenizer=train_dataset.tokenizer)

    train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size)
    
    # åˆå§‹åŒ–æ¨¡å‹
    vocab_size = train_dataset.tokenizer.vocab_size
   
    model = Seq2Seq(config=config,
                    device=device,
                    src_vocab_size=vocab_size,
                    tgt_vocab_size=vocab_size).to(device)
    best_bleu = 0.0
    loss_list = []
    bleu_list = []
    lr_list = []

    # åŠ è½½ckpt
    if not os.path.exists(config.checkpoint_path):
        print("Warning: checkpoint directory not found!")
        start_epoch = 0
        best_bleu = 0.0
    else:
        # åŠ è½½æ¨¡å‹
        print("===> Resume from checkpoint...")
        checkpoint = torch.load(config.checkpoint_path + 'checkpoint.pth')
        model.load_state_dict(checkpoint['model'])
        best_bleu = checkpoint['bleu']
        start_epoch = checkpoint['epoch']

    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    weight = torch.ones(train_dataset.tokenizer.vocab_size)
    weight[PAD_ID] = 0
    loss_function = nn.NLLLoss(weight, reduction='mean').to(device)
    optimizer = optim.SGD(model.parameters(), lr=config.lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

    msg = """
                +-------------------------------------------------------+
                |{:^47}|
                |=======================================================|
                |Optimizer: {:<10}  lr: {:<10}  Device: {}    |
                |-------------------------------------------------------|
                |Loss Function:{:<20}  Max_Epoch:{:<3}      |
                |=======================================================|
                |vocab size: {:<15}  batch size: {:<14}|
                |=======================================================|
                |è®­ç»ƒé›†é•¿åº¦: {:<17}  æµ‹è¯•é›†é•¿åº¦: {:<16}|
                |=======================================================|
                |{:<55}|
                +-------------------------------------------------------+
                """.format("æ¨¡å‹è®¾ç½®ä»¥åŠä»£ç è¿è¡Œç¯å¢ƒ", 'SGD', config.lr,
                           device,
                           str(loss_function), 
                           config.epoch, vocab_size, 
                           config.batch_size, len(train_dataset),
                           len(test_dataset), str(datetime.now()))
    print(msg)

    # è®­ç»ƒå’ŒéªŒè¯
    print("Start Epoch ====>\t", start_epoch)
    for i in range(start_epoch, config.epoch):
        train(model, train_loader, i + 1, config.epoch)
        if (i + 1) % config.num_val == 0:
            validation(model, dev_dataset, i)
    plot_carve(title="valid_bleu", save_path="../res_img/valid_bleu.png",
               x=len(bleu_list), y=bleu_list)
    plot_carve(title="train_loss", save_path="../res_img/train_loss.png", x=len(loss_list), y=loss_list)
    plot_carve(title="train_lr", save_path="../res_img/train_lr.png", x=len(lr_list), y=lr_list)

    predict(model, test_dataset)
```



### 6.2 éƒ¨åˆ†ç»“æœ

ç»è¿‡50æ¬¡è®­ç»ƒï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„éƒ¨åˆ†æ‰“å°è¾“å‡ºå¦‚å›¾ï¼Œåœ¨ç¬¬4è½®è¿­ä»£æ—¶éªŒè¯é›†ä¸Šçš„BLEU-4å€¼è¾¾åˆ°äº†0.6072ï¼Œç¬¬21è½®è¿­ä»£æ—¶è¾¾åˆ°äº†0.7099



![image-20230605143922789](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/image-20230605143922789.png)

![train_loss](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/train_loss.png)

![train_lr](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/train_lr.png)

![valid_bleu](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/valid_bleu.png)





## 7. Attentionå¯è§†åŒ–

Attentionå¯è§†åŒ–çš„ç†è®ºåŸºç¡€æ˜¯ä¸ºäº†è§£é‡Šå’Œç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†ä»»åŠ¡æ—¶çš„æ³¨æ„åŠ›åˆ†é…æœºåˆ¶ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸åœ¨å¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰ä»»åŠ¡æ—¶å…·æœ‰å¾ˆé«˜çš„æ€§èƒ½ï¼Œä½†å…¶å†…éƒ¨å·¥ä½œæœºåˆ¶å¾€å¾€æ˜¯é»‘ç›’å­ï¼Œéš¾ä»¥è§£é‡Šã€‚é€šè¿‡å¯è§†åŒ–Attentionï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹å¦‚ä½•åœ¨è¾“å…¥ä¸­é€‰æ‹©å’Œèšç„¦äºç›¸å…³çš„éƒ¨åˆ†ï¼Œä»¥ä¾¿ç”Ÿæˆæˆ–é¢„æµ‹è¾“å‡ºã€‚

Attentionå¯è§†åŒ–é€šè¿‡å°†æ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡ä¸è¾“å…¥å¯¹é½ï¼Œä»¥å›¾å½¢åŒ–æ–¹å¼æ˜¾ç¤ºæ¨¡å‹å¯¹è¾“å…¥çš„å…³æ³¨ç¨‹åº¦ã€‚è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°æ¨¡å‹åœ¨è¾“å…¥ä¸­ç€é‡å…³æ³¨çš„åŒºåŸŸå’Œç‰¹å¾ã€‚é€šè¿‡å¯è§†åŒ–Attentionï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºæ¨¡å‹åœ¨å¤„ç†ä¸åŒä»»åŠ¡æ—¶çš„æ³¨æ„åŠ›åˆ†å¸ƒæ¨¡å¼ï¼Œè¿›è€Œæ¨æ–­æ¨¡å‹å­¦ä¹ åˆ°çš„ç‰¹å¾å’Œå†³ç­–ä¾æ®ã€‚



```python
def visualize_attention(dataset, data_index=0):
    """Attentionå¯è§†åŒ–"""
    src, tgt, lex, _ = dataset[data_index]
    src = torch.as_tensor(src[np.newaxis, :]).to(device).transpose(0, 1)
    sentence, attention = model.predict(src)
    src_txt = list(map(lambda x: dataset.tokenizer.index_to_token(x),
                       src.flatten().cpu().numpy().tolist()[:10]))
    for i in range(len(src_txt)):
        if src_txt[i] == '[NAME]':
            src_txt[i] = lex[0]
        elif src_txt[i] == '[NEAR]':
            src_txt[i] = lex[1]
    sentence_txt = list(map(lambda x: dataset.tokenizer.index_to_token(x),
                            sentence))
    for i in range(len(src_txt)):
        if sentence_txt[i] == '[NAME]':
            sentence_txt[i] = lex[0]
        elif sentence_txt[i] == '[NEAR]':
            sentence_txt[i] = lex[1]

    # ç»˜åˆ¶çƒ­åŠ›å›¾
    ax = sns.heatmap(np.array(attention)[:, :10] * 100, cmap='YlGnBu')
    # è®¾ç½®åæ ‡è½´
    plt.yticks([i + 0.5 for i in range(len(sentence_txt))], labels=sentence_txt, rotation=360, fontsize=12)
    plt.xticks([i + 0.5 for i in range(len(src_txt))], labels=src_txt, fontsize=12)
    plt.show()
```



å¦‚å›¾æ˜¯æ ¹æ®Attentionæƒé‡ç»˜åˆ¶çš„çƒ­åŠ›å›¾ï¼Œå…¶ä¸­æ¨ªåæ ‡ä¸ºç»“æ„åŒ–æ–‡æœ¬ä¸­çš„å±æ€§å€¼ï¼Œåˆ†åˆ«å¯¹åº”nameã€é¤foodã€priceRange ã€customer ratingã€ familyFriendlyã€ areaã€nearã€ eatTypeï¼Œ0 åˆ™æ˜¯æœªç»™å®šå±æ€§å€¼ï¼›çºµåæ ‡è¡¨ç¤ºç¼–ç çš„ç»“æœå¥å­ç»“æœã€‚é¢œè‰²è¶Šæ·±çš„å—è¡¨ç¤ºå¯¹åº”çš„æ¨ªåæ ‡å¯¹äºæ¨ç†å‡ºçºµåæ ‡çš„å€¼æä¾›çš„å¸®åŠ©è¶Šå¤§ï¼Œä¾‹å¦‚å›¾ä¸­ no å¯¹äºæ¨ç†å‡ºlocatedã€nearä¸¤ä¸ªå•è¯çš„å¸®åŠ©å¾ˆå¤§ã€‚

![image-20230605161008503](https://typora-1308640872.cos-website.ap-beijing.myqcloud.com/img/image-20230605161008503.png)
